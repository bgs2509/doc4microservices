---
description: Prometheus metrics and Grafana dashboards for DDD/Hex microservices
globs: ["src/**/*.py", "infrastructure/observability/prometheus/**/*", "infrastructure/observability/grafana/**/*"]
alwaysApply: true
---

### Title
Metrics Rules (Prometheus + Grafana for DDD/Hex microservices)

### Version and Date
- Version: v1.0.0
- Updated: 2025-09-19
- Owner: rules/observability

# Metrics Rules

### Purpose
Define comprehensive metrics collection using Prometheus and visualization with Grafana for DDD/Hex microservices. Implements the "Golden Signals" methodology and service-specific metrics patterns.

### Scope
- Covers: application metrics, infrastructure metrics, business metrics, custom dashboards
- Integrates with: `logging_rules.mdc` (Request ID correlation), `observability_rules.mdc` (overall strategy)
- Service types: FastAPI, Aiogram, AsyncIO workers
- Excludes: Log aggregation (see `elk_rules.mdc`), tracing (see `tracing_rules.mdc`)

### Target Audience
Backend engineers, DevOps/SRE, performance engineers, monitoring specialists.

### Terms and Definitions
- **Golden Signals**: Latency, Traffic, Errors, Saturation (Google SRE methodology)
- **RED Metrics**: Rate, Errors, Duration (for request-driven services)
- **USE Metrics**: Utilization, Saturation, Errors (for resource monitoring)
- **Cardinality**: Number of unique time series for a metric
- **Scrape**: Prometheus pulling metrics from target endpoints

## Metrics Strategy

### Golden Signals Implementation

#### 1. Latency (Response Time)
**Metric Type**: Histogram
**Purpose**: Track request/job processing duration

```python
from prometheus_client import Histogram
from src.core.logging import get_current_request_id

# FastAPI request latency
http_request_duration = Histogram(
    'http_request_duration_seconds',
    'HTTP request processing duration',
    ['method', 'endpoint', 'status_code']
)

# Aiogram message processing latency
bot_message_duration = Histogram(
    'bot_message_duration_seconds',
    'Bot message processing duration',
    ['message_type', 'handler']
)

# AsyncIO job processing latency
worker_job_duration = Histogram(
    'worker_job_duration_seconds',
    'Worker job processing duration',
    ['job_type', 'queue']
)
```

#### 2. Traffic (Throughput)
**Metric Type**: Counter
**Purpose**: Track request/message/job volume

```python
from prometheus_client import Counter

# FastAPI request volume
http_requests_total = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status_code']
)

# Aiogram message volume
bot_messages_total = Counter(
    'bot_messages_total',
    'Total bot messages processed',
    ['message_type', 'user_type']
)

# AsyncIO job volume
worker_jobs_total = Counter(
    'worker_jobs_total',
    'Total worker jobs processed',
    ['job_type', 'status']
)
```

#### 3. Errors (Error Rate)
**Metric Type**: Counter (subset of traffic)
**Purpose**: Track error rates and types

```python
# HTTP errors (subset of http_requests_total with 4xx/5xx status)
http_errors_total = Counter(
    'http_errors_total',
    'Total HTTP errors',
    ['method', 'endpoint', 'status_code', 'error_type']
)

# Bot processing errors
bot_errors_total = Counter(
    'bot_errors_total',
    'Total bot processing errors',
    ['message_type', 'error_type']
)

# Worker job errors
worker_errors_total = Counter(
    'worker_errors_total',
    'Total worker job errors',
    ['job_type', 'error_type']
)
```

#### 4. Saturation (Resource Utilization)
**Metric Type**: Gauge
**Purpose**: Track resource usage and capacity

```python
from prometheus_client import Gauge

# Application-level saturation
active_connections = Gauge(
    'http_active_connections',
    'Current active HTTP connections'
)

bot_active_handlers = Gauge(
    'bot_active_handlers',
    'Current active bot message handlers'
)

worker_queue_size = Gauge(
    'worker_queue_size',
    'Current worker queue size',
    ['queue_name']
)

worker_active_jobs = Gauge(
    'worker_active_jobs',
    'Current active worker jobs',
    ['worker_type']
)
```

## Service-Specific Metrics Implementation

### FastAPI Service Metrics

#### HTTP Middleware for Automatic Instrumentation
```python
# src/core/metrics.py
import time
from typing import Callable
from fastapi import Request, Response
from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST
from src.core.logging import get_current_request_id, logger

# Golden Signals metrics
http_requests_total = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status_code', 'service']
)

http_request_duration = Histogram(
    'http_request_duration_seconds',
    'HTTP request processing duration',
    ['method', 'endpoint', 'service'],
    buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 2.5, 5.0, 10.0]
)

http_active_requests = Gauge(
    'http_active_requests',
    'Current active HTTP requests',
    ['service']
)

# Business metrics
user_registrations_total = Counter(
    'user_registrations_total',
    'Total user registrations',
    ['source']
)

api_key_validations_total = Counter(
    'api_key_validations_total',
    'Total API key validations',
    ['result']
)

class MetricsMiddleware:
    def __init__(self, app_name: str = "api_service"):
        self.app_name = app_name

    async def __call__(self, request: Request, call_next: Callable) -> Response:
        # Record request start
        start_time = time.time()
        request_id = get_current_request_id()

        # Track active requests
        http_active_requests.labels(service=self.app_name).inc()

        # Process request
        try:
            response = await call_next(request)
            status_code = response.status_code
        except Exception as e:
            # Handle unhandled exceptions
            logger.exception(f"metrics_middleware_error error={str(e)}")
            status_code = 500
            response = Response(status_code=500)
        finally:
            # Calculate duration
            duration = time.time() - start_time

            # Extract endpoint (remove query params and normalize)
            endpoint = request.url.path
            method = request.method

            # Record metrics
            http_requests_total.labels(
                method=method,
                endpoint=endpoint,
                status_code=status_code,
                service=self.app_name
            ).inc()

            http_request_duration.labels(
                method=method,
                endpoint=endpoint,
                service=self.app_name
            ).observe(duration)

            # Track active requests
            http_active_requests.labels(service=self.app_name).dec()

            # Log metrics for debugging
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(
                    f"metrics_recorded method={method} endpoint={endpoint} "
                    f"status={status_code} duration={duration:.3f}s"
                )

        return response

# FastAPI integration
from fastapi import FastAPI

def setup_metrics(app: FastAPI, service_name: str = "api_service") -> None:
    """Set up metrics middleware and endpoints for FastAPI app."""

    # Add metrics middleware
    app.middleware("http")(MetricsMiddleware(service_name))

    # Add metrics endpoint
    @app.get("/metrics")
    async def metrics():
        return Response(
            content=generate_latest(),
            media_type=CONTENT_TYPE_LATEST
        )

    # Add metrics health check
    @app.get("/health/metrics")
    async def metrics_health():
        try:
            # Verify metrics are being collected
            metrics_output = generate_latest()
            return {
                "status": "healthy",
                "metrics_size": len(metrics_output),
                "service": service_name
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "service": service_name
            }

# Usage in main.py
from src.core.metrics import setup_metrics

app = FastAPI()
setup_metrics(app, "api_service")
```

#### Business Logic Metrics
```python
# src/services/user_service.py
from src.core.metrics import user_registrations_total
from src.core.logging import logger, get_current_request_id

async def create_user(user_data: UserCreate) -> User:
    """Create user with metrics tracking."""
    request_id = get_current_request_id()

    try:
        # Business logic
        user = await user_repository.create(user_data)

        # Record business metric
        user_registrations_total.labels(source=user_data.source or "api").inc()

        logger.info(f"user_created user_id={user.id} source={user_data.source}")
        return user

    except Exception as e:
        logger.exception(f"user_creation_failed error={str(e)}")
        raise
```

### Aiogram Service Metrics

#### Bot Middleware for Message Processing
```python
# src/core/bot_metrics.py
import time
from typing import Dict, Any, Callable, Awaitable
from aiogram import BaseMiddleware
from aiogram.types import Message, Update
from prometheus_client import Counter, Histogram, Gauge
from src.core.logging import get_current_request_id, set_request_id, generate_request_id, logger

# Bot-specific metrics
bot_messages_total = Counter(
    'bot_messages_total',
    'Total bot messages processed',
    ['message_type', 'user_type', 'chat_type']
)

bot_message_duration = Histogram(
    'bot_message_duration_seconds',
    'Bot message processing duration',
    ['message_type', 'handler'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]
)

bot_active_handlers = Gauge(
    'bot_active_handlers',
    'Current active bot message handlers'
)

bot_errors_total = Counter(
    'bot_errors_total',
    'Total bot processing errors',
    ['message_type', 'error_type', 'handler']
)

# Business metrics for bot
bot_media_processed = Counter(
    'bot_media_processed_total',
    'Total media files processed',
    ['media_type', 'file_size_bucket']
)

bot_user_sessions = Gauge(
    'bot_user_sessions',
    'Current active user sessions'
)

class BotMetricsMiddleware(BaseMiddleware):
    """Comprehensive metrics middleware for Aiogram bot."""

    async def __call__(
        self,
        handler: Callable[[Update, Dict[str, Any]], Awaitable[Any]],
        event: Update,
        data: Dict[str, Any]
    ) -> Any:
        # Set up request context
        request_id = get_current_request_id() or generate_request_id("bot")
        set_request_id(request_id)

        start_time = time.time()
        message_type = "unknown"
        handler_name = "unknown"
        user_type = "unknown"
        chat_type = "unknown"

        # Extract message metadata
        if event.message:
            message = event.message
            message_type = self._get_message_type(message)
            user_type = "premium" if message.from_user and message.from_user.is_premium else "regular"
            chat_type = message.chat.type if message.chat else "unknown"
            handler_name = handler.__name__ if hasattr(handler, '__name__') else "unknown"

        # Track active handlers
        bot_active_handlers.inc()

        try:
            # Process the message
            result = await handler(event, data)

            # Record successful processing
            bot_messages_total.labels(
                message_type=message_type,
                user_type=user_type,
                chat_type=chat_type
            ).inc()

            # Track media processing if applicable
            if event.message and message_type in ["photo", "document", "video", "voice"]:
                file_size_bucket = self._get_file_size_bucket(event.message)
                bot_media_processed.labels(
                    media_type=message_type,
                    file_size_bucket=file_size_bucket
                ).inc()

            return result

        except Exception as e:
            # Record error
            error_type = type(e).__name__
            bot_errors_total.labels(
                message_type=message_type,
                error_type=error_type,
                handler=handler_name
            ).inc()

            logger.exception(
                f"bot_handler_error handler={handler_name} "
                f"message_type={message_type} error_type={error_type}"
            )
            raise

        finally:
            # Record processing duration
            duration = time.time() - start_time
            bot_message_duration.labels(
                message_type=message_type,
                handler=handler_name
            ).observe(duration)

            # Track active handlers
            bot_active_handlers.dec()

            # Log metrics
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(
                    f"bot_metrics_recorded message_type={message_type} "
                    f"handler={handler_name} duration={duration:.3f}s"
                )

    def _get_message_type(self, message: Message) -> str:
        """Determine message type for metrics."""
        if message.text:
            return "text"
        elif message.photo:
            return "photo"
        elif message.document:
            return "document"
        elif message.video:
            return "video"
        elif message.voice:
            return "voice"
        elif message.sticker:
            return "sticker"
        else:
            return "other"

    def _get_file_size_bucket(self, message: Message) -> str:
        """Get file size bucket for media metrics."""
        file_size = 0

        if message.photo and message.photo[-1].file_size:
            file_size = message.photo[-1].file_size
        elif message.document and message.document.file_size:
            file_size = message.document.file_size
        elif message.video and message.video.file_size:
            file_size = message.video.file_size
        elif message.voice and message.voice.file_size:
            file_size = message.voice.file_size

        # Bucket by file size
        if file_size == 0:
            return "unknown"
        elif file_size < 1024 * 1024:  # < 1MB
            return "small"
        elif file_size < 10 * 1024 * 1024:  # < 10MB
            return "medium"
        else:
            return "large"

# Setup in main.py
from aiogram import Dispatcher
from src.core.bot_metrics import BotMetricsMiddleware

async def setup_bot_metrics(dp: Dispatcher) -> None:
    """Set up bot metrics middleware."""
    dp.middleware.setup(BotMetricsMiddleware())
    logger.info("bot_metrics_setup middleware=BotMetricsMiddleware status=registered")
```

### AsyncIO Worker Service Metrics

#### Worker Metrics Implementation
```python
# src/core/worker_metrics.py
import time
import asyncio
from typing import Dict, Any, Optional
from prometheus_client import Counter, Histogram, Gauge, start_http_server
from src.core.logging import get_current_request_id, set_request_id, generate_request_id, logger

# Worker-specific metrics
worker_jobs_total = Counter(
    'worker_jobs_total',
    'Total worker jobs processed',
    ['job_type', 'queue', 'status']
)

worker_job_duration = Histogram(
    'worker_job_duration_seconds',
    'Worker job processing duration',
    ['job_type', 'queue'],
    buckets=[0.1, 0.5, 1.0, 5.0, 10.0, 30.0, 60.0, 300.0]
)

worker_queue_size = Gauge(
    'worker_queue_size',
    'Current worker queue size',
    ['queue_name']
)

worker_active_jobs = Gauge(
    'worker_active_jobs',
    'Current active worker jobs',
    ['worker_type']
)

worker_errors_total = Counter(
    'worker_errors_total',
    'Total worker job errors',
    ['job_type', 'error_type', 'queue']
)

class WorkerMetrics:
    """Worker metrics collection and management."""

    def __init__(self, worker_type: str):
        self.worker_type = worker_type

    async def process_job_with_metrics(
        self,
        job_data: Dict[str, Any],
        job_processor: callable,
        queue_name: str = "default"
    ) -> Any:
        """Process job with comprehensive metrics tracking."""

        # Set up request context
        request_id = job_data.get('request_id') or generate_request_id("worker")
        set_request_id(request_id)

        job_type = job_data.get('type', 'unknown')
        start_time = time.time()

        # Track active jobs
        worker_active_jobs.labels(worker_type=self.worker_type).inc()

        try:
            # Process the job
            result = await job_processor(job_data)

            # Record successful processing
            worker_jobs_total.labels(
                job_type=job_type,
                queue=queue_name,
                status='success'
            ).inc()

            logger.info(
                f"worker_job_completed job_type={job_type} "
                f"queue={queue_name} worker_type={self.worker_type}"
            )

            return result

        except Exception as e:
            # Record error
            error_type = type(e).__name__
            worker_jobs_total.labels(
                job_type=job_type,
                queue=queue_name,
                status='error'
            ).inc()

            worker_errors_total.labels(
                job_type=job_type,
                error_type=error_type,
                queue=queue_name
            ).inc()

            logger.exception(
                f"worker_job_error job_type={job_type} "
                f"error_type={error_type} queue={queue_name}"
            )
            raise

        finally:
            # Record processing duration
            duration = time.time() - start_time
            worker_job_duration.labels(
                job_type=job_type,
                queue=queue_name
            ).observe(duration)

            # Track active jobs
            worker_active_jobs.labels(worker_type=self.worker_type).dec()

            # Log metrics
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(
                    f"worker_metrics_recorded job_type={job_type} "
                    f"duration={duration:.3f}s queue={queue_name}"
                )

    def update_queue_size(self, queue_name: str, size: int) -> None:
        """Update queue size metric."""
        worker_queue_size.labels(queue_name=queue_name).set(size)

# Usage in worker main.py
from src.core.worker_metrics import WorkerMetrics
from prometheus_client import start_http_server

async def main():
    """Worker main function with metrics."""

    # Start metrics server
    start_http_server(8001)  # Different port for each worker type
    logger.info("worker_metrics_server started=true port=8001")

    # Initialize worker metrics
    worker_metrics = WorkerMetrics("image_processor")

    # Example job processing
    async def process_image_job(job_data: Dict[str, Any]) -> Dict[str, Any]:
        # Actual image processing logic
        await asyncio.sleep(2)  # Simulate processing
        return {"status": "completed", "output_url": "s3://bucket/processed.jpg"}

    # Process jobs with metrics
    while True:
        try:
            # Get job from queue (pseudo-code)
            job_data = await get_next_job_from_queue()
            if job_data:
                await worker_metrics.process_job_with_metrics(
                    job_data,
                    process_image_job,
                    queue_name="image_processing"
                )
            else:
                await asyncio.sleep(1)  # No jobs available

        except Exception as e:
            logger.exception(f"worker_main_error error={str(e)}")
            await asyncio.sleep(5)  # Wait before retrying

if __name__ == "__main__":
    asyncio.run(main())
```

## Prometheus Configuration

### Prometheus Config File
```yaml
# infrastructure/observability/prometheus/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alerts/*.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  # FastAPI services
  - job_name: 'api_service'
    static_configs:
      - targets: ['api_service:8000']
    metrics_path: '/metrics'
    scrape_interval: 15s

  # Bot services (if they expose metrics endpoint)
  - job_name: 'bot_service'
    static_configs:
      - targets: ['bot_service:8001']
    metrics_path: '/metrics'
    scrape_interval: 15s

  # Worker services
  - job_name: 'worker_service'
    static_configs:
      - targets: ['worker_service:8001']
    metrics_path: '/metrics'
    scrape_interval: 15s

  # Infrastructure services
  - job_name: 'postgres-exporter'
    static_configs:
      - targets: ['postgres-exporter:9187']

  - job_name: 'redis-exporter'
    static_configs:
      - targets: ['redis-exporter:9121']

  - job_name: 'rabbitmq-exporter'
    static_configs:
      - targets: ['rabbitmq:15692']

  # Prometheus itself
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
```

## Grafana Dashboard Configuration

### Service Overview Dashboard
```json
{
  "dashboard": {
    "title": "Microservices Overview",
    "panels": [
      {
        "title": "Request Rate (RPS)",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{service}} - {{method}} {{endpoint}}"
          }
        ]
      },
      {
        "title": "Error Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total{status_code=~\"4..|5..\"}[5m]) / rate(http_requests_total[5m])",
            "legendFormat": "{{service}} Error Rate"
          }
        ]
      },
      {
        "title": "Response Time (95th percentile)",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "{{service}} - {{method}} {{endpoint}}"
          }
        ]
      },
      {
        "title": "Active Connections",
        "type": "singlestat",
        "targets": [
          {
            "expr": "sum(http_active_requests)",
            "legendFormat": "Active Requests"
          }
        ]
      }
    ]
  }
}
```

## Alerting Rules

### Critical Alerts
```yaml
# infrastructure/observability/prometheus/alerts/critical.yml
groups:
  - name: critical
    rules:
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "Service {{ $labels.job }} on {{ $labels.instance }} has been down for more than 1 minute."

      - alert: HighErrorRate
        expr: |
          (
            rate(http_requests_total{status_code=~"5.."}[5m]) /
            rate(http_requests_total[5m])
          ) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate on {{ $labels.service }}"
          description: "Error rate is {{ $value | humanizePercentage }} for {{ $labels.service }}"

      - alert: HighLatency
        expr: |
          histogram_quantile(0.95,
            rate(http_request_duration_seconds_bucket[5m])
          ) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency on {{ $labels.service }}"
          description: "95th percentile latency is {{ $value }}s for {{ $labels.service }}"
```

## Best Practices

### Metric Naming Conventions
- Use snake_case: `http_requests_total`, not `httpRequestsTotal`
- Include units in name: `_seconds`, `_bytes`, `_total`
- Use meaningful prefixes: `http_`, `bot_`, `worker_`
- Keep cardinality low: avoid high-cardinality labels like user_id

### Label Guidelines
- Use consistent label names across metrics
- Avoid high-cardinality labels (user IDs, request IDs)
- Use meaningful label values
- Group related metrics with common labels

### Performance Considerations
- Monitor metric cardinality regularly
- Use recording rules for expensive queries
- Implement metric retention policies
- Consider sampling for high-volume metrics

## Verification Commands

```bash
# Check metrics endpoints
curl http://localhost:8000/metrics  # FastAPI service
curl http://localhost:8001/metrics  # Worker service

# Verify Prometheus targets
curl http://localhost:9090/api/v1/targets

# Test Grafana API
curl -u admin:admin123 http://localhost:3000/api/health

# Check metric cardinality
curl -s http://localhost:9090/api/v1/label/__name__/values | jq '.data | length'
```

## Troubleshooting

### Common Issues
1. **High cardinality**: Remove high-cardinality labels
2. **Missing metrics**: Check service registration and endpoints
3. **Stale metrics**: Verify scrape intervals and target health
4. **Performance impact**: Implement sampling and optimize queries

### Monitoring Metrics Health
```python
# Add to health check endpoints
async def metrics_health_check():
    """Verify metrics collection is healthy."""
    try:
        # Check recent metric updates
        from prometheus_client import REGISTRY

        metrics_count = len(list(REGISTRY._collector_to_names.keys()))
        return {
            "status": "healthy",
            "metrics_registered": metrics_count,
            "last_update": time.time()
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }
```

---

## Summary

This metrics implementation provides comprehensive monitoring for your microservices using the Golden Signals methodology. The setup integrates seamlessly with your existing logging system and provides production-ready dashboards and alerting capabilities.

**Key Features**:
- Golden Signals implementation (Latency, Traffic, Errors, Saturation)
- Service-specific metrics for FastAPI, Aiogram, and AsyncIO workers
- Request ID correlation with existing logging system
- Production-ready Prometheus and Grafana configuration
- Comprehensive alerting rules and dashboards
- Performance-optimized with cardinality management