---
description: Production-grade logging standard for DDD/Hex microservices
globs: ["src/**/*.py"]
alwaysApply: true
---

### Title
Logging Rules (DDD/Hex microservices)

### Version and Date
- Version: v2.0.0
- Updated: 2025-09-19
- Owner: rules/architecture

# Logging Rules

### Purpose
Establish a single, production‑grade logging standard for DDD/Hex‑aligned microservices deployed on VPS (Docker Compose) and cloud, observable via OpenTelemetry/ELK/CloudWatch.

### Scope
- Covers: formatting, context propagation, middleware, handlers/formatters, sanitization, levels/policies, observability integration, performance/rotation, and verification.
- Excludes: general microservice guidance (`ms_best_practices_rules.mdc`) and FastAPI HTTP specifics (`fastapi_rules.mdc`).

### Target Audience
Engineers (backend, platform), Tech Leads, DevOps/SRE, library/adapter authors.

### Use Cases
HTTP APIs (FastAPI), AsyncIO.run(), Telegram bots (Aiogram), LLM pipelines (LangChain), DBs (PostgreSQL/ChromaDB/MongoDB), queues (RabbitMQ), cache (Redis), servers (Uvicorn, fronted by Nginx).

### Terms and Definitions
- Request ID: stable correlation id stored in ContextVar and emitted by formatter
- Trace ID: W3C Trace Context trace identifier extracted from current span
- Problem Details: RFC 7807 error structure used by HTTP layer (referenced; not duplicated)

## MCP Server Context 7 Compliance
Requires Context 7 terminology, structure, and artifacts for consistent organization-wide logging.

### Context 7 Artifacts and Conventions
Defines common vocabulary, ID patterns, and cross-layer consistency for DDD/Hex boundaries and logging primitives.

### Rules
1) MUST: Configure only the Root Logger in `src/core/logging.py::setup_logging()` with identical stdout and file handlers using one formatter.
2) MUST: Use `ContextVar` request id; generate via `generate_request_id(prefix, **context)`; no manual prefixes in messages — the formatter adds `[request_id]`.
3) MUST: Propagate `X-Request-ID` and set it on responses via `RequestIDMiddleware`; log concise access lines.
4) MUST: Append `trace_id` when OpenTelemetry span context is available; never block on OTEL.
5) MUST: Sanitize secrets before logging: mask keys in `SENSITIVE_KEYS` including `password`, `api_key`, `token`, `secret`, `key`, `authorization`.
6) MUST: Avoid logging large payloads; truncate previews and guard heavy debug under `logger.isEnabledFor(DEBUG)`.
7) MUST: Rotate file `logs/logs.log` at 1 MB with 3 backups by default; directory must exist.
8) SHOULD: Use consistent key=value pairs; include units; keep messages short and structured.
9) SHOULD: Respect DDD/Hex boundaries — domain logs business events; adapters log technical details; routers do not implement logging primitives.
10) SHOULD: Centralize configuration via environment variables (`LOG_LEVEL`, `LOG_SERVICE_NAME`, `ENV`, `OTEL_EXPORTER_OTLP_ENDPOINT`).
11) MAY: Sample DEBUG logs in PROD; increase verbosity in DEV/STAGE per environment policy.
12) MUST: Initialization and bootstrapping — Call `setup_logging()` exactly once at the beginning of the `main()` function after configuration is loaded; MUST NOT call `setup_logging()` at import time in libraries (no module‑level side effects in `src/core/logging.py` or elsewhere); MUST ensure no duplicate handler attachment on hot‑reload/re‑import.
13) MUST: Add DEBUG logging at function entry and exit in DEV environment — log function name, key parameters (sanitized), and execution duration for all business logic methods.
14) MUST: Use lazy logging for expensive operations — guard debug computations behind `logger.isEnabledFor(logging.DEBUG)` to avoid performance impact.
15) SHOULD: Log variable states at critical decision points — include key variable values when they affect business logic flow.
16) MUST: Log all configuration parameters from `src/core/config.py` during class initialization in DEBUG level — sanitize sensitive values before logging.
17) SHOULD: Actively use DEBUG level in DEV environment for comprehensive debugging — set LOG_LEVEL=DEBUG by default in development.

## Core Principles
Guarantee predictability, standardization, safety, performance, and traceability.

### Format and Consistency
Enforce a unified f-string message style: `message key=value ...`; do not manually prefix with request id — the formatter appends `[request_id]` automatically.

### Minimum Useful Information
Mandatory fields: `request_id` prefix in brackets, concise message, key=value pairs with units and precision.

## Correlation and Context Propagation
Rules for correlating events across HTTP, queues, workers, and data stores.

### Request ID
Only generate via `generate_request_id(prefix, **context)`; store in `ContextVar`; include in every log line.

### Traceparent / Trace ID
Honor W3C Trace Context; connect logs to OpenTelemetry `trace_id`; propagate via headers/metadata.

### Cross-Boundary Propagation
Automatically propagate `request_id`/trace across HTTP middleware, MQ headers, and background workers.

## Log Message Format
Standard f-string style, consistent keys, units, and truncated previews.

### Keys and Conventions
Use clear, consistent keys such as `user_id`, `chat_id`, `duration`, `status`, `model`, `tokens`, `ai_time` in `key=value` format.

### Length Limits
Truncate long values (e.g., previews with `[:100]...`); avoid large payloads in logs.

## Logging Levels and Policies
Define semantics across environments: DEBUG (diagnostics), INFO (business events), WARNING (anomalies), ERROR/EXCEPTION (failures with stack trace).

### Environment Policies
- DEV: rich logs, DEBUG required (default LOG_LEVEL=DEBUG); comprehensive function entry/exit logging; all config parameters logged at initialization; variable state logging at decision points
- STAGE: INFO baseline, DEBUG sampled; reduced function logging; critical path variable logging only
- PROD: INFO/WARNING/ERROR; DEBUG strictly sampled; minimal function logging for performance

## DDD/Hex Layer Guidelines
Respect boundaries for domain, application, and adapters/infrastructure.

### Domain Layer
Only log business events; no infrastructural/internal transport details.

### Application Layer
Log start, key steps, outcomes, and timings; aggregate metrics where high volume.

### Adapters/Infrastructure
HTTP, brokers, DB, cache, filesystem: include technical details, timings, codes, retries; never log secrets.

## Framework- and Service-Specific Guidance
Concise rules per dependency.

### FastAPI
Add middleware for `request_id`; log lifecycle (startup/shutdown), exceptions, `/health`; set `X-Request-ID` in responses.

### Aiogram & AsyncIO().run
Use comprehensive middleware for Request ID propagation, timing, and media detection; normalize `user_id`/`chat_id`; mask sensitive input; log rate-limit scenarios, media file metadata (photo, document, video, voice), processing timing with `duration=X.XXXs`, and error handling with sanitization.

### LangChain
Use callbacks: `on_llm_start/end/error`; log latency, tokens, model; mask prompts/responses.

### Pydantic
Log validation outcomes without secrets; prefer summarizing fields over raw payloads.

### SQLAlchemy/asyncpg
Log timings, slow-query warnings, pool metrics; avoid dumping sensitive parameters; disable SQL echo in PROD.

### RabbitMQ (aio-pika)
Propagate `x-request-id` and `traceparent` headers; log `ack/nack/requeue` with standardized format (e.g., `status=ack`, `error=invalid_payload`), DLQ, retries/backoff.

### Redis (redis.asyncio)
Aggregate hits/misses; warn on large keys or blocking ops; track TTL and key growth.


### PostgreSQL
Use slow-query thresholds, set `application_name`; align client logs with server-side settings.

### MongoDB (Motor)
Use `slowms` profiling; log sizes and warnings for scans.

### ChromaDB
Log upsert/query counts, batch sizes, embedding dims, search params, and fallbacks.

### Uvicorn
Use custom log config/access format; log errors, reduce noisy warnings.

### Nginx (front proxy)
Ensure `X-Request-ID` is forwarded; log upstream timings; avoid duplicate access logging with app server.

## Configuration and Environments
Environment-driven configuration and profiles.

### Environment Variables
`LOG_LEVEL`, `SERVICE_NAME`, `SERVICE_VERSION`, `ENV`, `OTEL_EXPORTER_OTLP_ENDPOINT`.

### Profiles: DEV/STAGE/PROD
Control verbosity, sampling, retention, and destinations.

## Performance and Volume Control
Avoid expensive work when not needed; sample high-volume debug logs.

### Lazy Logging
Guard expensive computations behind level checks. MUST use `logger.isEnabledFor(logging.DEBUG)` before:
- Complex data serialization for logging
- Large object inspection
- Expensive string formatting operations
- Configuration parameter extraction and formatting
- Variable state collection and formatting

### Rotation and Retention
Write to `logs/logs.log`, rotate at 1 MB, set Docker log-driver limits; monitor volume.

## Security and Privacy
Never log secrets; sanitize inputs and structured payloads.

### Sensitive Data
Mask `password`, `api_key`, `token`, `secret`, `key`, `authorization` and similar fields.

### Sanitization
Apply a sanitizer before logging structures; use truncated previews.

## Errors and Safe Fallback
Ensure reliable logging under failure conditions.

### Exceptions
Use `logger.exception` with context; include error type and codes in key=value style.

### Safe Fallback
Allow `print()` only as a last resort for critical events; configure Ruff to ignore rule T201.

## Observability and Telemetry
Integrate logs with traces and metrics.

### OpenTelemetry
Enable traces/metrics/logs; export via OTLP; include resource attributes; correlate `request_id` with `trace_id`.

### ELK / CloudWatch
Ship logs with Fluent Bit/Vector/awslogs; configure retention/ILM; mask PII at shipper level if needed.

## Docker Compose and VPS Operations
Container-friendly logging and shipping.

### Log Drivers
Use `json-file` with size/rotation limits or ship to an external collector; prefix by service.

### Health/Readiness
Log `/health`, startup/shutdown, and version info for operational visibility.

## Quality and Compliance
Static analysis and tests make logging reliable.

### Lint/Format
Use Ruff, allow `T201` for fallback logging, enforce import grouping and full typing.

### Testing
Cover `request_id` generation/propagation, sanitization, rotation, and error handlers.

## Versioning Policy
Evolve schemas safely.

### Semantic Changes
Classify breaking vs non-breaking changes to log shape; communicate and migrate parsers.

### Agent Compatibility
Maintain backward compatibility with log shippers and parsers.

## Anti-Patterns
Avoid bad practices that hinder analysis.

### Forbidden Formats
No logs without `request_id`; no manual ID generation; no ad-hoc structures or `extra`-only payloads.

### Redundancy and Duplication
Avoid logging the same event at multiple layers or multiple times.

## Glossary and References
Shared terms and links.

### Terms
`request_id`, `trace_id`, `span_id`, `DLQ`, `slow query`, `sampling`, `ILM`.

### References
W3C Trace Context; OpenTelemetry; vendor and internal logging guides.

## Consistency Check
All sections align on formatter‑added `[request_id]` prefix, ContextVar propagation, mandatory middleware, 1 MB rotation, and OTEL/ELK/CloudWatch compatibility; conflicts resolved to use Root Logger only and forbid manual prefixes.

### Examples

Note: Examples are illustrative. The canonical implementation and handlers live only in `src/core/logging.py`. Do not manually prefix messages with `[{request_id}]`; the formatter decorates lines automatically. Use Root Logger configured via `setup_logging()`.

```1:60:src/core/logging.py
"""Logging configuration for the Microservice."""

import os
import logging
import sys
import time
import uuid
from collections.abc import Awaitable, Callable
from contextvars import ContextVar
from logging.handlers import RotatingFileHandler
from pathlib import Path
from typing import Any

from fastapi import Request

try:
    from opentelemetry import trace as ot_trace  # type: ignore[import-not-found]
except ImportError:  # pragma: no cover - optional dependency
    ot_trace = None  # type: ignore[assignment]

from src.core.config import settings

# Context 7: Request ID management
request_id_var: ContextVar[str | None] = ContextVar("request_id", default=None)


def set_request_id(request_id: str) -> None:
    """Store the current request id in a ContextVar."""
    request_id_var.set(request_id)


def get_current_request_id() -> str | None:
    """Return the current request id from the ContextVar if present."""
    return request_id_var.get()

def generate_request_id(prefix: str = "req", **kwargs: str | float | bool) -> str:
    """Generate a Context 7 compliant request id.

    The format is: ``{prefix}_{k1_v1}_{k2_v2}_<unix_ts>_<short_uid>`` when context
    kwargs are provided, otherwise ``{prefix}_<unix_ts>_<short_uid>``.
    """
    ts = int(time.time())
    uid = str(uuid.uuid4())[:8]
    if kwargs:
        ctx = "_".join(f"{k}_{v}" for k, v in kwargs.items())
        return f"{prefix}_{ctx}_{ts}_{uid}"
    return f"{prefix}_{ts}_{uid}"

# Optional: add OpenTelemetry trace id

def get_current_trace_id() -> str | None:
    try:
        from opentelemetry import trace
        span = trace.get_current_span()
        span_ctx = span.get_span_context()
        if span_ctx and span_ctx.trace_id:
            return f"{span_ctx.trace_id:032x}"
    except Exception:
        return None
    return None

SENSITIVE_KEYS = {"password", "api_key", "token", "secret", "key", "authorization"}

def sanitize_for_logging(data: dict[str, Any]) -> dict[str, Any]:
    sanitized = dict(data)
    for k in list(sanitized.keys()):
        if k.lower() in SENSITIVE_KEYS or sum(
            [1 for a in SENSITIVE_KEYS if a in k.lower()],
        ):
            sanitized[k] = k.lower()[:3] + "***MASKED***"
    return sanitized

class RequestTraceFormatter(logging.Formatter):
    def format(self, record: logging.LogRecord) -> str:
        rid = get_current_request_id()
        tid = get_current_trace_id()
        base = super().format(record)
        prefix = f"[{rid}]" if rid else "[no-request-id]"
        if tid:
            return f"{prefix} {base} trace_id={tid}"
        return f"{prefix} {base}"


def _parse_rotation_to_bytes(rotation: str) -> int:
    """Parse rotation string to bytes (e.g., '1MB' -> 1048576)."""
    if rotation.endswith('MB'):
        return int(float(rotation[:-2]) * 1024 * 1024)
    elif rotation.endswith('KB'):
        return int(float(rotation[:-2]) * 1024)
    elif rotation.endswith('GB'):
        return int(float(rotation[:-2]) * 1024 * 1024 * 1024)
    else:
        return int(rotation)  # Assume bytes if no suffix

def _parse_retention_to_backups(retention: str) -> int:
    """Parse retention string to backup count (e.g., '3' -> 3)."""
    return int(retention)

def setup_logging() -> None:
    """Initialize logging configuration. Call this exactly once at application startup."""
    os.makedirs("logs", exist_ok=True)

    root_logger = logging.getLogger()
    level = getattr(logging, settings.log_level.upper(), logging.INFO)
    root_logger.setLevel(level)

    formatter = RequestTraceFormatter(
        fmt="%(asctime)s %(levelname)s %(name)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(level)
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)

    # File handler with rotation (1 MB per rules)
    file_handler = RotatingFileHandler(
        settings.log_file,
        maxBytes=_parse_rotation_to_bytes(settings.log_rotation),
        backupCount=_parse_retention_to_backups(settings.log_retention),
        encoding="utf-8",
    )
    file_handler.setLevel(level)
    file_handler.setFormatter(formatter)
    root_logger.addHandler(file_handler)

logger = logging.getLogger(__name__)
# NOTE: setup_logging() is NOT called here to comply with rule 12
# It must be called exactly once at application startup in main() function

# Safe critical fallback

def safe_log_critical(message: str, request_id: str | None = None) -> None:
    try:
        rid = request_id or get_current_request_id()
        logger.critical(message)
    except Exception:
        print(f"CRITICAL: {message}")  # noqa: T201

# FastAPI RequestID Middleware - MUST be in src/core/logging.py per rule 0
class RequestIDMiddleware(BaseHTTPMiddleware):
    """FastAPI middleware for Request ID propagation and access logging.

    This middleware MUST be defined only in src/core/logging.py per rule 0.
    Application code MUST import and use this class, not define its own.
    """
    async def dispatch(self, request: Request, call_next):
        rid = request.headers.get("X-Request-ID") or generate_request_id("req")
        set_request_id(rid)
        start = time.time()
        response = await call_next(request)
        duration = time.time() - start
        response.headers["X-Request-ID"] = rid
        logger.info(f"http_request method={request.method} path={request.url.path} status={response.status_code} duration={duration:.3f}s")
        return response

```

```python
# FastAPI: request-id middleware, lifespan, health
from __future__ import annotations

import time
from contextlib import asynccontextmanager
from typing import Any

from fastapi import FastAPI, Request

# IMPORTANT: RequestIDMiddleware MUST be imported from src/core/logging.py per rule 0
from src.core.logging import logger, generate_request_id, set_request_id, get_current_request_id, setup_logging, RequestIDMiddleware

@asynccontextmanager
async def lifespan(_app: FastAPI):
    setup_logging()  # single call after settings are loaded
    rid = generate_request_id("startup")
    set_request_id(rid)
    logger.info("service_startup service=ai_speed_classifier")
    yield
    rid = generate_request_id("shutdown")
    set_request_id(rid)
    logger.info("service_shutdown service=ai_speed_classifier")

app = FastAPI(lifespan=lifespan)
app.add_middleware(RequestIDMiddleware)

@app.get("/health")
async def health() -> dict[str, Any]:
    rid = get_current_request_id()
    logger.debug("health_check endpoint=/health")
    return {"status": "healthy", "request_id": rid, "timestamp": time.time()}

```

```python
# Aiogram: comprehensive request context middleware with timing, error handling, and media detection
from __future__ import annotations

import time
from typing import Awaitable, Callable, Dict, Any
from aiogram import BaseMiddleware, types
from aiogram.types import Message

from src.core.logging import logger, generate_request_id, set_request_id, get_current_request_id, sanitize_for_logging

class RequestContextMiddleware(BaseMiddleware):
    """Comprehensive Aiogram middleware for Request ID propagation, timing, and logging.

    Aligns with aiogram_rules.mdc requirements:
    - Generates and propagates Request ID for full tracing
    - Logs timing information for performance monitoring
    - Handles media file detection and metadata extraction
    - Provides proper error handling with sanitization
    - Integrates with logging_rules.mdc standards
    """

    async def __call__(
        self,
        handler: Callable[[types.TelegramObject, Dict[str, Any]], Awaitable[Any]],
        event: types.TelegramObject,
        data: Dict[str, Any],
    ) -> Any:
        # Extract user and chat information
        user_id = getattr(getattr(event, "from_user", None), "id", None)
        chat_id = getattr(getattr(event, "chat", None), "id", None)

        # Generate or use existing Request ID with context
        rid = get_current_request_id() or generate_request_id(
            "tg",
            user_id=user_id or "unknown",
            chat_id=chat_id or "unknown"
        )
        set_request_id(rid)

        # Start timing
        start_time = time.time()

        # Extract additional metadata for comprehensive logging
        event_type = type(event).__name__
        message_type = None
        media_info = {}

        # Enhanced media detection for aiogram_rules.mdc compliance
        if isinstance(event, Message):
            if event.photo:
                message_type = "photo"
                media_info = {
                    "file_count": len(event.photo),
                    "largest_file_id": event.photo[-1].file_id if event.photo else None,
                    "file_size": event.photo[-1].file_size if event.photo else None,
                }
            elif event.document:
                message_type = "document"
                media_info = {
                    "file_id": event.document.file_id,
                    "file_name": event.document.file_name,
                    "file_size": event.document.file_size,
                    "mime_type": event.document.mime_type,
                }
            elif event.video:
                message_type = "video"
                media_info = {
                    "file_id": event.video.file_id,
                    "duration": event.video.duration,
                    "file_size": event.video.file_size,
                }
            elif event.voice:
                message_type = "voice"
                media_info = {
                    "file_id": event.voice.file_id,
                    "duration": event.voice.duration,
                    "file_size": event.voice.file_size,
                }
            elif event.text:
                message_type = "text"
                # Sanitize text content for logging (truncate and mask sensitive data)
                text_preview = (event.text or "")[:100]
                media_info = {"text_length": len(event.text or "")}
            else:
                message_type = "other"

        # Log incoming update with comprehensive metadata
        log_data = {
            "type": event_type,
            "user_id": user_id,
            "chat_id": chat_id,
        }

        if message_type:
            log_data["message_type"] = message_type

        if media_info:
            # Sanitize media info before logging
            sanitized_media = sanitize_for_logging(media_info)
            log_data.update(sanitized_media)

        # Format log message with key=value pairs
        log_parts = [f"{k}={v}" for k, v in log_data.items() if v is not None]
        logger.info(f"tg_update_received {' '.join(log_parts)}")

        try:
            # Process the update through the handler
            result = await handler(event, data)

            # Calculate processing time
            duration = time.time() - start_time

            # Log successful processing
            logger.info(f"tg_update_processed duration={duration:.3f}s status=success")

            return result

        except Exception as e:
            # Calculate processing time for failed requests
            duration = time.time() - start_time

            # Log error with sanitized error information
            error_info = {
                "error_type": type(e).__name__,
                "error_message": str(e)[:200],  # Truncate long error messages
            }
            sanitized_error = sanitize_for_logging(error_info)

            logger.exception(
                f"tg_update_failed duration={duration:.3f}s status=error "
                f"error_type={sanitized_error['error_type']} "
                f"error_message={sanitized_error['error_message']}"
            )

            # Re-raise the exception to maintain normal error handling
            raise

# Example of middleware registration with dispatcher
# from aiogram import Dispatcher, Router
#
# async def setup_middleware(dp: Dispatcher) -> None:
#     """Setup Request ID middleware for all updates."""
#     dp.middleware.setup(RequestContextMiddleware())
#     logger.info("tg_middleware_setup middleware=RequestContextMiddleware status=registered")
#
# # Usage with router
# async def setup_router_middleware(router: Router) -> None:
#     """Setup Request ID middleware for specific router."""
#     router.middleware(RequestContextMiddleware())
#     logger.info("tg_router_middleware_setup middleware=RequestContextMiddleware status=registered")

```

```python
# LangChain: callback handler for LLM operations
from __future__ import annotations

import time
from typing import Any, Dict, List

from langchain_core.callbacks import BaseCallbackHandler
from src.core.logging import logger, generate_request_id, set_request_id, get_current_request_id

class LCLoggingHandler(BaseCallbackHandler):
    def __init__(self) -> None:
        self.start_ts: float | None = None

    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **_: Any) -> None:
        rid = get_current_request_id() or generate_request_id("ai")
        set_request_id(rid)
        self.start_ts = time.time()
        model = serialized.get("id") or serialized.get("name")
        preview = (prompts[0] if prompts else "")[:100]
        logger.debug(f"llm_start model={model} prompt_preview={preview}...")

    def on_llm_end(self, response, **_: Any) -> None:
        rid = get_current_request_id()
        duration = (time.time() - self.start_ts) if self.start_ts else 0.0
        token_usage = getattr(getattr(response, "llm_output", {}), "get", lambda *_: None)("token_usage")  # type: ignore
        logger.info(f"llm_end duration={duration:.3f}s tokens={token_usage}")

    def on_llm_error(self, error: Exception, **_: Any) -> None:
        rid = get_current_request_id()
        logger.exception(f"llm_error error={str(error)}")

```

```python
# SQLAlchemy + asyncpg: query timings and slow-query warnings
from __future__ import annotations

import time
from sqlalchemy.ext.asyncio import create_async_engine, AsyncEngine
from sqlalchemy import event
from src.core.logging import logger, generate_request_id, get_current_request_id

engine: AsyncEngine = create_async_engine(
    "postgresql+asyncpg://user:pass@localhost:5432/dbname",
    pool_size=10,
    max_overflow=10,
    pool_pre_ping=True,
    echo=False,
)

@event.listens_for(engine.sync_engine, "before_cursor_execute")
def before_cursor_execute(conn, cursor, statement, parameters, context, executemany):  # noqa: D401
    context._query_start_time = time.time()

@event.listens_for(engine.sync_engine, "after_cursor_execute")
def after_cursor_execute(conn, cursor, statement, parameters, context, executemany):  # noqa: D401
    rid = get_current_request_id() or generate_request_id("db")
    total = (time.time() - context._query_start_time) if getattr(context, "_query_start_time", None) else 0.0
    if total > 0.5:
        sql_prev = " ".join(statement.split())[:200]
        logger.warning(f"slow_query time={total:.3f}s sql=\"{sql_prev}\"")

```

```python
# RabbitMQ (aio-pika): publish and consume with headers propagation
from __future__ import annotations

import json
import aio_pika
from src.core.logging import logger, generate_request_id, get_current_request_id, set_request_id

async def publish_example() -> None:
    rid = get_current_request_id() or generate_request_id("mq")
    conn = await aio_pika.connect_robust("amqp://guest:guest@localhost/")
    async with conn:
        ch = await conn.channel()
        ex = await ch.declare_exchange("photos.events", aio_pika.ExchangeType.TOPIC, durable=True)
        body = json.dumps({"event": "user.created"}).encode()
        msg = aio_pika.Message(body=body, headers={"x-request-id": rid}, content_type="application/json")
        await ex.publish(msg, routing_key="user.created")
        logger.info(f"mq_publish exchange=events rk=user.created size={len(body)}b")

async def consume_example(message: aio_pika.IncomingMessage) -> None:
    rid = message.headers.get("x-request-id") or generate_request_id("mq")
    set_request_id(rid)
    logger.info(f"mq_received rk={message.routing_key} size={len(message.body)}b")
    await message.ack()

```

```python
# Redis (redis.asyncio): thin wrapper with aggregated logging
from __future__ import annotations

from redis import asyncio as redis
from src.core.logging import logger, generate_request_id, get_current_request_id

class RedisClient:
    def __init__(self, url: str) -> None:
        self._url = url
        self._client: redis.Redis | None = None

    async def connect(self) -> None:
        self._client = redis.from_url(self._url, decode_responses=True)

    async def get(self, key: str) -> str | None:
        rid = get_current_request_id() or generate_request_id("cache")
        val = await self._client.get(key)  # type: ignore[union-attr]
        logger.debug(f"cache_get key={key} hit={val is not None}")
        return val

    async def set(self, key: str, value: str, ex: int | None = None) -> None:
        rid = get_current_request_id() or generate_request_id("cache")
        await self._client.set(key, value, ex=ex)  # type: ignore[union-attr]
        logger.info(f"cache_set key={key} ttl={ex}")

```


```python
# MongoDB (Motor): simple wrapper with request-id
from __future__ import annotations

from motor.motor_asyncio import AsyncIOMotorClient
from src.core.logging import logger, generate_request_id, get_current_request_id

client = AsyncIOMotorClient("mongodb://localhost:27017/?appName=ai_speed_classifier")
db = client["appdb"]

async def find_user(user_id: str) -> dict | None:
    rid = get_current_request_id() or generate_request_id("mongo")
    doc = await db.users.find_one({"_id": user_id})
    logger.info(f"mongo_find collection=users id={user_id} found={doc is not None}")
    return doc

```

```python
# ChromaDB: add/query with concise metrics
from __future__ import annotations

from chromadb import PersistentClient
from src.core.logging import logger, generate_request_id, get_current_request_id

client = PersistentClient(path="./chroma")
collection = client.get_or_create_collection(name="docs")

def add_embeddings(ids: list[str], embeddings: list[list[float]], metadatas: list[dict] | None = None) -> None:
    rid = get_current_request_id() or generate_request_id("vec")
    collection.add(ids=ids, embeddings=embeddings, metadatas=metadatas)
    dim = len(embeddings[0]) if embeddings else 0
    logger.info(f"chroma_add count={len(ids)} dim={dim}")


def search(query_emb: list[float], k: int = 5):
    rid = get_current_request_id() or generate_request_id("vec")
    res = collection.query(query_embeddings=[query_emb], n_results=k)
    results = len(res.get("ids", [[]])[0]) if res.get("ids") else 0
    logger.info(f"chroma_query k={k} results={results}")
    return res

```

```python
# Uvicorn: start with shared logging config
from __future__ import annotations

import uvicorn
from src.core.logging import logger

if __name__ == "__main__":
    logger.info("startup launching uvicorn app=src.main:app")
    uvicorn.run("src.main:app", host="0.0.0.0", port=8000, log_level="info", proxy_headers=True, forwarded_allow_ips="*")

```

```python
# PostgreSQL: application_name via SQLAlchemy URL
from __future__ import annotations

from sqlalchemy.ext.asyncio import create_async_engine

engine = create_async_engine(
    "postgresql+asyncpg://user:pass@localhost:5432/dbname?application_name=ai_speed_classifier",
    pool_size=10,
    max_overflow=10,
)

```

```python
# Pydantic: validation logging with sanitization
from __future__ import annotations

from pydantic import BaseModel, ValidationError
from src.core.logging import logger, generate_request_id, get_current_request_id, sanitize_for_logging

class User(BaseModel):
    id: int
    name: str
    api_key: str | None = None  # sensitive

def create_user(data: dict) -> User:
    rid = get_current_request_id() or generate_request_id("val")
    try:
        user = User(**data)
        logger.info(f"user_validated id={user.id} name_len={len(user.name)}")
        return user
    except ValidationError as e:
        payload = sanitize_for_logging(data)
        logger.error(f"validation_failed fields_count={len(e.errors())} payload_preview={str(payload)[:120]}...")
        raise

```

```python
# Lazy debug example (avoid expensive work unless DEBUG is enabled)
from __future__ import annotations

from src.core.logging import logger, get_current_request_id, generate_request_id

def expensive_operation() -> str:
    return "x" * 10000

def maybe_debug() -> None:
    rid = get_current_request_id() or generate_request_id("dbg")
    if logger.isEnabledFor(10):  # logging.DEBUG
        data = expensive_operation()
        logger.debug(f"debug_data size={len(data)} preview={data[:100]}")

```

```python
# Safe fallback usage example
from __future__ import annotations

from src.core.logging import safe_log_critical, generate_request_id, set_request_id

rid = generate_request_id("critical")
set_request_id(rid)
safe_log_critical("storage_unavailable operation=data_processing", request_id=rid)

```

```python
# Nginx cooperation (application-side check of forwarded headers)
from __future__ import annotations

from fastapi import Request

EXPECTED_HEADER = "X-Request-ID"

def assert_forwarded_request_id(request: Request) -> None:
    # Validate that the proxy (e.g., Nginx) forwarded the request id
    assert EXPECTED_HEADER in request.headers, "X-Request-ID header missing from proxy"

```

### Priorities and Compatibility
- Depends on: `ms_best_practices_rules.mdc` (system-wide standard).
- Does not duplicate: General microservice practices (see `ms_best_practices_rules.mdc`) and domain-specific HTTP rules.
- Conflicts: Canonical Logging Implementation here is the source of truth for logging; update other docs if conflicts arise.

## Canonical Logging Implementation (Single Source of Truth)

This section defines a single, canonical logging implementation for all services to guarantee identical, complete, and safe logs across stdout (Docker) and the rotating file. It also bans duplication of logging logic anywhere else in the codebase.

### 0) Single Source of Truth and No Duplication
- All logging primitives MUST live only in `src/core/logging.py`.
- Forbidden to define/duplicate formatters, middleware, request-id utilities, sanitizers, or fallback logging outside `src/core/logging.py`.
- **CRITICAL**: `RequestIDMiddleware` MUST be defined only in `src/core/logging.py` and imported by application code.
- Application code MUST only use `logging.getLogger(__name__)` and utilities imported from `src/core/logging.py`.
- Handlers/formatters MUST be attached only in `setup_logging()`; adding handlers elsewhere is forbidden.

### 1) Output Channels and Identical Payload
- Two mandatory outputs:
  - stdout via `StreamHandler(sys.stdout)` (captured by Docker)
  - file `logs/logs.log` via `RotatingFileHandler`
- Both handlers MUST use the exact same formatter so each log line is byte-identical in stdout and the file.
- Default rotation policy: `maxBytes=1_000_000`, `backupCount=3` (may be overridden in config; defaults remain unchanged).

### 2) Global Root Logger Configuration
- `setup_logging()` MUST configure the Root Logger, not a named logger.
- All handlers MUST be added only to the Root Logger; named loggers MUST not attach handlers.
- `propagate=True` is expected; do not bypass Root Logger formatting.
- Application code MUST NOT manually prefix messages with `"[{request_id}]"`; the formatter handles request/trace decoration.

### 3) Log Line Shape and Required Fields
- Each log line is:
  - A prefix `"[{request_id}]"` or `"[no-request-id]"`,
  - The standard message (internal format),
  - followed by `trace_id=<32hex>` when available.
- Use concise `key=value` pairs in the message body; timestamps/level/logger are provided by the formatter.
- Example:
  - `[req_user_42_1700000000_ab12cd34] service_startup service=my_svc trace_id=1af...9c0`

### 4) Context and Correlation: Request ID and Trace ID
- Request ID:
  - Backed by `ContextVar`.
  - Utilities: `set_request_id`, `get_current_request_id`, `generate_request_id(prefix="req", **context)`.
- Trace ID:
  - If OpenTelemetry is enabled, obtain `trace_id` from the current span context (W3C Trace Context) and append it automatically in the formatter.

### 5) Secret Sanitization (No Leaks)
- Mandatory set: `SENSITIVE_KEYS = {"password", "api_key", "token", "secret", "key", "authorization"}`.
- Utility `sanitize_for_logging(d: dict) -> dict` MUST be used before logging any structured payload to mask sensitive values.

### 6) Safe Critical Fallback
- Provide `safe_log_critical(message: str, request_id: str | None = None)`:
  - Attempt to log via the configured logger.
  - On I/O failures, fall back to `print(...)` as a last resort.

### 7) Mandatory FastAPI RequestID Middleware and Access Logs
- **MUST**: Define `RequestIDMiddleware` class only in `src/core/logging.py` (per rule 0).
- **MUST**: Application code import `RequestIDMiddleware` from `src/core/logging.py`, not define its own.
- The middleware MUST:
  - Read or generate `X-Request-ID`.
  - Store in `ContextVar`.
  - Add `X-Request-ID` response header.
  - Log an access line at INFO with `method`, `path`, `status`, `duration=<sec>s`.
- Access log MUST NOT manually insert `"[{request_id}]"` or `trace_id`; the formatter adds these automatically.

### 8) Canonical API Surface in `src/core/logging.py`
The following symbols and signatures are required:
- `request_id_var: ContextVar[str | None]`
- `set_request_id(request_id: str) -> None`
- `get_current_request_id() -> str | None`
- `generate_request_id(prefix: str = "req", **kwargs: str | float | bool) -> str`
- `get_current_trace_id() -> str | None`
- `SENSITIVE_KEYS: set[str]`
- `sanitize_for_logging(data: dict[str, Any]) -> dict[str, Any]`
- `setup_logging() -> None`  (Root Logger + stdout/file handlers + unified formatter)
- `safe_log_critical(message: str, request_id: str | None = None) -> None`
- `class RequestIDMiddleware` (ASGI-compatible; sets `X-Request-ID`, logs access with timings)

### 9) Defaults and Rotation
- File path MUST be `logs/logs.log`; directory MUST be created on setup.
- Environment variables: `LOG_LEVEL`, `SERVICE_NAME`, `ENV`, `OTEL_EXPORTER_OTLP_ENDPOINT` (optional).
- Default level: `INFO`.
- Default rotation: `1 MB` and `3` backups (profiles may override, but defaults stand).

### 10) Testing and CI Enforcement
- Mandatory tests MUST verify:
  - Any `logging.getLogger("x").info("...")` line includes `[request_id]`/`[no-request-id]`, and `trace_id=...` when OTel is active.
  - Identical log lines appear in both stdout and `logs/logs.log`.
  - `RequestIDMiddleware` sets `X-Request-ID` and logs access with timings.
  - `sanitize_for_logging` masks all keys in `SENSITIVE_KEYS`.
  - `safe_log_critical` works under logger failures.
- CI MUST fail if:
  - Additional handlers/formatters are attached outside `setup_logging()`.
  - Logging primitives are defined outside `src/core/logging.py`.

### 11) Extension Policy
- When integrating brokers/DBs/clients, never handcraft request/trace prefixes; rely on the formatter.
- For heavy debug data, guard with `logger.isEnabledFor(logging.DEBUG)`.

### 12) Environment Profiles
- DEV/STAGE/PROD may override level and file location.
- The formatter, context decoration (request/trace), sanitization policy, and the two mandatory channels (stdout + file) MUST remain unchanged across profiles.

### Verification
- How to verify:
  - Ensure handlers/formatter configured only via `src/core/logging.py::setup_logging()` and Root Logger handlers are used.
  - Confirm exactly one non‑test call site initializes logging (no module‑level calls in libraries):
    - `rg "\\bsetup_logging\\(" --glob '!**/tests/**'`
    - `rg "setup_logging\\(\\)" src/core/logging.py` must return no import‑time invocation.
  - Run service and check logs in both stdout and `logs/logs.log` are byte-identical per line.
  - Confirm `X-Request-ID` is set on HTTP responses and access lines include method, path, status, duration.
  - Verify `sanitize_for_logging` masks keys in `SENSITIVE_KEYS`.
- Success criteria:
  - Single explicit call in entry point; no calls at import time.
  - No extra handlers outside Root Logger; no manual prefixes in messages; identical outputs across channels; sanitization effective.


### Changes
- 2025-01-27 v1.5.0: Add comprehensive DEBUG logging principles: function entry/exit logging, variable state logging, configuration parameter logging, enhanced DEV environment policies, and lazy logging guidelines.
- 2025-09-05 v1.4.0: Add initialization rule requiring single explicit `setup_logging()` call at entry point; examples and verification updated.
- 2025-09-05 v1.3.1: Fix repository code citations to match `src/core/logging.py`; no normative rule changes.
- 2025-09-05 v1.3.0: Align with Cursor authoring standard: added front matter, split Purpose/Scope, added Terms, numbered Rules, Priorities/Compatibility, structured Examples with repo citation, and updated Changes.
- 2025-09-05 v1.2.0: Switched to Root Logger config, removed manual prefixes in examples, fixed imports to `src/core/logging`, deduplicated Environment Profiles, updated consistency note.
