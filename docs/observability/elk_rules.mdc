---
description: ELK stack (Elasticsearch, Logstash, Kibana) integration for DDD/Hex microservices
globs: ["infrastructure/observability/elk/**/*", "src/**/*.py"]
alwaysApply: true
---

### Title
ELK Rules (Elasticsearch + Logstash + Kibana for DDD/Hex microservices)

### Version and Date
- Version: v1.0.0
- Updated: 2025-09-19
- Owner: rules/observability

# ELK Rules

### Purpose
Implement comprehensive log aggregation, analysis, and visualization using the ELK stack for DDD/Hex microservices. Enhances the existing logging foundation from `logging_rules.mdc` with centralized log management.

### Scope
- Covers: log aggregation, parsing, enrichment, indexing, visualization, alerting
- Builds on: `logging_rules.mdc` (leverages existing structured logs and Request ID system)
- Integrates with: `observability_rules.mdc` (overall strategy), `tracing_rules.mdc` (trace correlation)
- Excludes: Application logging patterns (see `logging_rules.mdc`), metrics (see `metrics_rules.mdc`)

### Target Audience
DevOps/SRE engineers, backend engineers, security analysts, monitoring specialists.

### Terms and Definitions
- **Index**: Elasticsearch logical grouping of documents (like database tables)
- **Document**: Individual log entry stored in Elasticsearch
- **Pipeline**: Logstash processing sequence for log transformation
- **Mapping**: Elasticsearch schema definition for fields
- **Shard**: Elasticsearch data distribution unit

## ELK Stack Architecture

### Component Overview

#### 1. Elasticsearch (Storage & Search)
- **Purpose**: Distributed search and analytics engine for log storage
- **Role**: Index, store, and search log documents
- **Data Model**: JSON documents with dynamic schema
- **Scaling**: Horizontal scaling via shards and replicas

#### 2. Logstash (Processing & Enrichment)
- **Purpose**: Data processing pipeline for log transformation
- **Role**: Parse, filter, enrich, and route log data
- **Plugins**: Input, filter, and output plugins for various data sources
- **Performance**: Multi-threaded processing with configurable workers

#### 3. Kibana (Visualization & Analysis)
- **Purpose**: Web interface for data exploration and visualization
- **Role**: Search logs, create dashboards, set up alerts
- **Features**: Real-time dashboards, alerting, machine learning

#### 4. Filebeat (Log Shipping)
- **Purpose**: Lightweight log shipper for collecting and forwarding logs
- **Role**: Monitor log files and Docker containers
- **Benefits**: Backpressure handling, guaranteed delivery

## Docker Compose Configuration

### Complete ELK Stack Setup
```yaml
# infrastructure/observability/docker-compose.elk.yml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.15.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - xpack.security.enrollment.enabled=false
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - network.host=0.0.0.0
      - http.cors.enabled=true
      - http.cors.allow-origin="*"
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
      - ./infrastructure/observability/elk/elasticsearch/config:/usr/share/elasticsearch/config
    networks:
      - observability
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  logstash:
    image: docker.elastic.co/logstash/logstash:8.15.0
    container_name: logstash
    volumes:
      - ./infrastructure/observability/elk/logstash/pipeline:/usr/share/logstash/pipeline:ro
      - ./infrastructure/observability/elk/logstash/config:/usr/share/logstash/config:ro
      - ./logs:/usr/share/logstash/logs:ro
    ports:
      - "5044:5044"  # Beats input
      - "5000:5000/tcp"  # TCP input
      - "5000:5000/udp"  # UDP input
      - "9600:9600"  # API
    environment:
      - LS_JAVA_OPTS=-Xms1g -Xmx1g
      - PIPELINE_WORKERS=2
    networks:
      - observability
    depends_on:
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9600/_node/stats || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  kibana:
    image: docker.elastic.co/kibana/kibana:8.15.0
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=changeme
    volumes:
      - ./infrastructure/observability/elk/kibana/config:/usr/share/kibana/config:ro
    networks:
      - observability
    depends_on:
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5601/api/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  filebeat:
    image: docker.elastic.co/beats/filebeat:8.15.0
    container_name: filebeat
    user: root
    volumes:
      - ./infrastructure/observability/elk/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - ./logs:/var/log/services:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - filebeat_data:/usr/share/filebeat/data
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - LOGSTASH_HOSTS=logstash:5044
    networks:
      - observability
    depends_on:
      logstash:
        condition: service_healthy

volumes:
  elasticsearch_data:
  filebeat_data:

networks:
  observability:
    external: true
```

## Filebeat Configuration

### Comprehensive Log Collection
```yaml
# infrastructure/observability/elk/filebeat/filebeat.yml
filebeat.inputs:
  # Application logs from mounted logs directory
  - type: log
    enabled: true
    paths:
      - /var/log/services/logs.log*
    fields:
      service: "microservices"
      log_type: "application"
    fields_under_root: true
    multiline.pattern: '^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}'
    multiline.negate: true
    multiline.match: after

  # Docker container logs
  - type: container
    enabled: true
    paths:
      - /var/lib/docker/containers/*/*.log
    processors:
      - add_docker_metadata:
          host: "unix:///var/run/docker.sock"
      - decode_json_fields:
          fields: ["message"]
          target: ""
          overwrite_keys: true

# Output to Logstash for processing
output.logstash:
  hosts: ["logstash:5044"]
  worker: 2
  compression_level: 3
  bulk_max_size: 2048

# Logging configuration
logging.level: info
logging.to_files: true
logging.files:
  path: /var/log/filebeat
  name: filebeat
  keepfiles: 7
  permissions: 0644

# Monitoring
monitoring.enabled: true
monitoring.elasticsearch:
  hosts: ["http://elasticsearch:9200"]

# Processors for log enhancement
processors:
  - add_host_metadata:
      when.not.contains.tags: forwarded
  - add_cloud_metadata: ~
  - add_docker_metadata: ~
  - timestamp:
      field: "@timestamp"
      layouts:
        - '2006-01-02T15:04:05.000Z'
        - '2006-01-02T15:04:05Z'
      test:
        - '2023-01-01T12:00:00.000Z'
```

## Logstash Configuration

### Pipeline for Log Processing
```ruby
# infrastructure/observability/elk/logstash/pipeline/microservices.conf
input {
  beats {
    port => 5044
  }
}

filter {
  # Parse application logs (from logging_rules.mdc format)
  if [log_type] == "application" {
    # Parse the log format: [request_id] timestamp level logger - message trace_id=xxx
    grok {
      match => {
        "message" => "\[%{DATA:request_id}\] %{TIMESTAMP_ISO8601:log_timestamp} %{LOGLEVEL:log_level} %{DATA:logger_name} - %{GREEDYDATA:log_message}"
      }
    }

    # Extract trace_id if present
    if [log_message] {
      grok {
        match => {
          "log_message" => "%{DATA:message_content}(?: trace_id=%{DATA:trace_id})?"
        }
        overwrite => [ "log_message" ]
      }

      mutate {
        rename => { "message_content" => "log_message" }
      }
    }

    # Parse key=value pairs from log message
    kv {
      source => "log_message"
      field_split => " "
      value_split => "="
      target => "log_fields"
    }

    # Convert timestamp
    date {
      match => [ "log_timestamp", "yyyy-MM-dd HH:mm:ss" ]
      target => "@timestamp"
    }

    # Add service name based on container
    if [container] and [container][name] {
      mutate {
        add_field => { "service_name" => "%{[container][name]}" }
      }
    }

    # Extract error information
    if [log_level] == "ERROR" {
      mutate {
        add_field => { "is_error" => true }
      }

      # Extract error type from log_fields if available
      if [log_fields][error_type] {
        mutate {
          add_field => { "error_type" => "%{[log_fields][error_type]}" }
        }
      }
    }

    # Performance metrics extraction
    if [log_fields][duration] {
      mutate {
        convert => { "[log_fields][duration]" => "float" }
        add_field => { "duration_seconds" => "%{[log_fields][duration]}" }
      }
    }

    if [log_fields][status] {
      mutate {
        add_field => { "response_status" => "%{[log_fields][status]}" }
      }
    }

    # Business metrics extraction
    if [log_fields][user_id] {
      mutate {
        add_field => { "user_id" => "%{[log_fields][user_id]}" }
      }
    }

    if [log_fields][chat_id] {
      mutate {
        add_field => { "chat_id" => "%{[log_fields][chat_id]}" }
      }
    }
  }

  # Parse Docker container logs
  if [container] {
    # Add container metadata
    mutate {
      add_field => {
        "container_id" => "%{[container][id]}"
        "container_image" => "%{[container][image]}"
      }
    }

    # Try to parse JSON logs from containers
    if [message] =~ /^\{.*\}$/ {
      json {
        source => "message"
        target => "json_log"
      }

      # Extract common fields from JSON logs
      if [json_log][level] {
        mutate {
          add_field => { "log_level" => "%{[json_log][level]}" }
        }
      }

      if [json_log][timestamp] {
        date {
          match => [ "[json_log][timestamp]", "ISO8601" ]
          target => "@timestamp"
        }
      }
    }
  }

  # Enrich with environment information
  mutate {
    add_field => {
      "environment" => "${ENVIRONMENT:development}"
      "cluster" => "${CLUSTER_NAME:local}"
    }
  }

  # GeoIP enrichment for IP addresses (if present in logs)
  if [log_fields][client_ip] {
    geoip {
      source => "[log_fields][client_ip]"
      target => "geoip"
    }
  }

  # Remove unnecessary fields
  mutate {
    remove_field => [ "[beat]", "[prospector]", "[input]", "[host][name]" ]
  }
}

output {
  # Output to Elasticsearch with dynamic index naming
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "microservices-%{+YYYY.MM.dd}"
    template_name => "microservices"
    template => "/usr/share/logstash/templates/microservices-template.json"
    template_overwrite => true
  }

  # Debug output (remove in production)
  stdout {
    codec => rubydebug {
      metadata => false
    }
  }
}
```

### Elasticsearch Index Template
```json
{
  "index_patterns": ["microservices-*"],
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 1,
    "index.refresh_interval": "30s",
    "index.mapping.total_fields.limit": 2000
  },
  "mappings": {
    "properties": {
      "@timestamp": {
        "type": "date"
      },
      "request_id": {
        "type": "keyword",
        "fields": {
          "text": {
            "type": "text"
          }
        }
      },
      "trace_id": {
        "type": "keyword"
      },
      "log_level": {
        "type": "keyword"
      },
      "logger_name": {
        "type": "keyword"
      },
      "log_message": {
        "type": "text",
        "analyzer": "standard"
      },
      "service_name": {
        "type": "keyword"
      },
      "container_id": {
        "type": "keyword"
      },
      "container_image": {
        "type": "keyword"
      },
      "is_error": {
        "type": "boolean"
      },
      "error_type": {
        "type": "keyword"
      },
      "duration_seconds": {
        "type": "float"
      },
      "response_status": {
        "type": "keyword"
      },
      "user_id": {
        "type": "keyword"
      },
      "chat_id": {
        "type": "keyword"
      },
      "environment": {
        "type": "keyword"
      },
      "cluster": {
        "type": "keyword"
      },
      "geoip": {
        "properties": {
          "location": {
            "type": "geo_point"
          },
          "country_name": {
            "type": "keyword"
          },
          "city_name": {
            "type": "keyword"
          }
        }
      },
      "log_fields": {
        "type": "object",
        "dynamic": true
      }
    }
  }
}
```

## Kibana Configuration

### Index Patterns and Data Views
```json
{
  "objects": [
    {
      "id": "microservices-*",
      "type": "index-pattern",
      "attributes": {
        "title": "microservices-*",
        "timeFieldName": "@timestamp",
        "fields": "[{\"name\":\"@timestamp\",\"type\":\"date\",\"searchable\":true,\"aggregatable\":true},{\"name\":\"request_id\",\"type\":\"string\",\"searchable\":true,\"aggregatable\":true},{\"name\":\"trace_id\",\"type\":\"string\",\"searchable\":true,\"aggregatable\":true},{\"name\":\"log_level\",\"type\":\"string\",\"searchable\":true,\"aggregatable\":true},{\"name\":\"service_name\",\"type\":\"string\",\"searchable\":true,\"aggregatable\":true},{\"name\":\"log_message\",\"type\":\"string\",\"searchable\":true,\"aggregatable\":false},{\"name\":\"duration_seconds\",\"type\":\"number\",\"searchable\":true,\"aggregatable\":true},{\"name\":\"response_status\",\"type\":\"string\",\"searchable\":true,\"aggregatable\":true},{\"name\":\"is_error\",\"type\":\"boolean\",\"searchable\":true,\"aggregatable\":true},{\"name\":\"error_type\",\"type\":\"string\",\"searchable\":true,\"aggregatable\":true}]"
      }
    }
  ]
}
```

### Kibana Dashboards

#### Service Overview Dashboard
```json
{
  "objects": [
    {
      "id": "microservices-overview",
      "type": "dashboard",
      "attributes": {
        "title": "Microservices Overview",
        "hits": 0,
        "description": "Overview of microservices logs and metrics",
        "panelsJSON": "[{\"version\":\"8.15.0\",\"gridData\":{\"x\":0,\"y\":0,\"w\":24,\"h\":15,\"i\":\"1\"},\"panelIndex\":\"1\",\"embeddableConfig\":{\"title\":\"Log Levels Over Time\"},\"panelRefName\":\"panel_1\"},{\"version\":\"8.15.0\",\"gridData\":{\"x\":24,\"y\":0,\"w\":24,\"h\":15,\"i\":\"2\"},\"panelIndex\":\"2\",\"embeddableConfig\":{\"title\":\"Errors by Service\"},\"panelRefName\":\"panel_2\"},{\"version\":\"8.15.0\",\"gridData\":{\"x\":0,\"y\":15,\"w\":48,\"h\":15,\"i\":\"3\"},\"panelIndex\":\"3\",\"embeddableConfig\":{\"title\":\"Request Duration Distribution\"},\"panelRefName\":\"panel_3\"}]",
        "timeRestore": false,
        "kibanaSavedObjectMeta": {
          "searchSourceJSON": "{\"query\":{\"match_all\":{}},\"filter\":[]}"
        }
      }
    }
  ]
}
```

#### Error Analysis Dashboard
```json
{
  "objects": [
    {
      "id": "error-analysis",
      "type": "dashboard",
      "attributes": {
        "title": "Error Analysis",
        "hits": 0,
        "description": "Detailed error analysis and patterns",
        "panelsJSON": "[{\"version\":\"8.15.0\",\"gridData\":{\"x\":0,\"y\":0,\"w\":24,\"h\":15,\"i\":\"1\"},\"panelIndex\":\"1\",\"embeddableConfig\":{\"title\":\"Error Rate by Service\"},\"panelRefName\":\"panel_1\"},{\"version\":\"8.15.0\",\"gridData\":{\"x\":24,\"y\":0,\"w\":24,\"h\":15,\"i\":\"2\"},\"panelIndex\":\"2\",\"embeddableConfig\":{\"title\":\"Top Error Types\"},\"panelRefName\":\"panel_2\"},{\"version\":\"8.15.0\",\"gridData\":{\"x\":0,\"y\":15,\"w\":48,\"h\":20,\"i\":\"3\"},\"panelIndex\":\"3\",\"embeddableConfig\":{\"title\":\"Error Details Table\"},\"panelRefName\":\"panel_3\"}]"
      }
    }
  ]
}
```

## Integration with Existing Systems

### Request ID Correlation
The ELK stack leverages the excellent Request ID system from `logging_rules.mdc`:

```bash
# Search logs by Request ID across all services
GET microservices-*/_search
{
  "query": {
    "term": {
      "request_id": "req_1700000000_ab12cd34"
    }
  },
  "sort": [
    { "@timestamp": { "order": "asc" } }
  ]
}
```

### Trace ID Integration
Correlate logs with distributed traces from `tracing_rules.mdc`:

```bash
# Find all logs for a specific trace
GET microservices-*/_search
{
  "query": {
    "term": {
      "trace_id": "1af7b5c6d8e9f0a1b2c3d4e5f6a7b8c9"
    }
  }
}
```

### Application Integration
Enhance application logging for better ELK integration:

```python
# src/core/elk_integration.py
import json
from typing import Dict, Any
from src.core.logging import logger, get_current_request_id
from src.core.tracing import get_current_trace_id

def log_business_event(
    event_type: str,
    event_data: Dict[str, Any],
    user_id: str = None,
    additional_fields: Dict[str, Any] = None
) -> None:
    """Log business events in ELK-friendly format."""

    request_id = get_current_request_id()
    trace_id = get_current_trace_id()

    # Structure data for ELK parsing
    log_data = {
        "event_type": event_type,
        "user_id": user_id,
        **event_data,
        **(additional_fields or {})
    }

    # Create key=value format for Logstash KV filter
    kv_pairs = []
    for key, value in log_data.items():
        if value is not None:
            kv_pairs.append(f"{key}={value}")

    message = f"business_event {' '.join(kv_pairs)}"

    logger.info(message)

# Usage examples
async def create_user_example():
    """Example of business event logging."""

    log_business_event(
        "user_registration",
        {
            "registration_source": "api",
            "user_type": "premium",
            "registration_method": "email"
        },
        user_id="12345",
        additional_fields={
            "campaign_id": "summer2023",
            "referrer": "google"
        }
    )

async def process_payment_example():
    """Example of payment processing logging."""

    log_business_event(
        "payment_processed",
        {
            "amount": "29.99",
            "currency": "USD",
            "payment_method": "stripe",
            "status": "success"
        },
        user_id="12345",
        additional_fields={
            "subscription_type": "monthly",
            "payment_gateway": "stripe"
        }
    )
```

## Alerting and Monitoring

### Elasticsearch Watcher Alerts
```json
{
  "trigger": {
    "schedule": {
      "interval": "1m"
    }
  },
  "input": {
    "search": {
      "request": {
        "search_type": "query_then_fetch",
        "indices": ["microservices-*"],
        "body": {
          "query": {
            "bool": {
              "must": [
                {
                  "term": {
                    "log_level": "ERROR"
                  }
                },
                {
                  "range": {
                    "@timestamp": {
                      "gte": "now-5m"
                    }
                  }
                }
              ]
            }
          },
          "aggs": {
            "error_count": {
              "value_count": {
                "field": "log_level"
              }
            }
          }
        }
      }
    }
  },
  "condition": {
    "compare": {
      "ctx.payload.aggregations.error_count.value": {
        "gt": 10
      }
    }
  },
  "actions": {
    "send_email": {
      "email": {
        "to": ["alerts@company.com"],
        "subject": "High Error Rate Detected",
        "body": "More than 10 errors detected in the last 5 minutes. Please investigate."
      }
    }
  }
}
```

### Kibana Alerting Rules
```json
{
  "name": "High Error Rate Alert",
  "consumer": "alerts",
  "enabled": true,
  "alertTypeId": ".index-threshold",
  "schedule": {
    "interval": "1m"
  },
  "params": {
    "index": ["microservices-*"],
    "timeField": "@timestamp",
    "aggType": "count",
    "termSize": 5,
    "termField": "service_name.keyword",
    "thresholdComparator": ">",
    "threshold": [10],
    "timeWindowSize": 5,
    "timeWindowUnit": "m",
    "filterQuery": {
      "bool": {
        "must": [
          {
            "term": {
              "log_level": "ERROR"
            }
          }
        ]
      }
    }
  },
  "actions": [
    {
      "id": "slack-action",
      "group": "threshold met",
      "params": {
        "message": "High error rate detected in {{context.group}} - {{context.value}} errors in the last 5 minutes"
      }
    }
  ]
}
```

## Performance Optimization

### Index Lifecycle Management (ILM)
```json
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_size": "10gb",
            "max_docs": 10000000,
            "max_age": "7d"
          },
          "set_priority": {
            "priority": 100
          }
        }
      },
      "warm": {
        "min_age": "7d",
        "actions": {
          "set_priority": {
            "priority": 50
          },
          "allocate": {
            "number_of_replicas": 0
          }
        }
      },
      "cold": {
        "min_age": "30d",
        "actions": {
          "set_priority": {
            "priority": 0
          }
        }
      },
      "delete": {
        "min_age": "90d"
      }
    }
  }
}
```

### Elasticsearch Configuration Tuning
```yaml
# infrastructure/observability/elk/elasticsearch/config/elasticsearch.yml
cluster.name: "microservices-logs"
node.name: "elasticsearch-1"

# Memory settings
bootstrap.memory_lock: true

# Network settings
network.host: 0.0.0.0
http.port: 9200

# Discovery settings
discovery.type: single-node

# Index settings
action.auto_create_index: "+microservices-*,+.kibana*,-.security*"

# Performance tuning
index.refresh_interval: 30s
index.number_of_shards: 1
index.number_of_replicas: 1

# Thread pools
thread_pool.search.queue_size: 1000
thread_pool.write.queue_size: 1000

# Circuit breaker settings
indices.breaker.total.limit: 70%
indices.breaker.fielddata.limit: 40%
indices.breaker.request.limit: 40%
```

## Verification and Testing

### Health Check Scripts
```bash
#!/bin/bash
# infrastructure/observability/elk/scripts/health_check.sh

echo "Checking ELK Stack Health..."

# Check Elasticsearch
echo "Elasticsearch:"
curl -s "http://localhost:9200/_cluster/health?pretty" | jq '.status'

# Check Logstash
echo "Logstash:"
curl -s "http://localhost:9600/_node/stats" | jq '.pipeline.events.in'

# Check Kibana
echo "Kibana:"
curl -s "http://localhost:5601/api/status" | jq '.status.overall.state'

# Check indices
echo "Indices:"
curl -s "http://localhost:9200/_cat/indices/microservices-*?v"

# Test log ingestion
echo "Testing log ingestion..."
echo '{"test": true, "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%S.%3NZ)'", "message": "ELK health check"}' | \
  curl -X POST "http://localhost:9200/microservices-test/_doc" \
  -H "Content-Type: application/json" \
  -d @-

echo "Health check complete!"
```

### Query Examples
```bash
# Search for errors in the last hour
curl -X GET "localhost:9200/microservices-*/_search?pretty" -H 'Content-Type: application/json' -d'
{
  "query": {
    "bool": {
      "must": [
        {"term": {"log_level": "ERROR"}},
        {"range": {"@timestamp": {"gte": "now-1h"}}}
      ]
    }
  },
  "sort": [{"@timestamp": {"order": "desc"}}],
  "size": 10
}'

# Aggregate errors by service
curl -X GET "localhost:9200/microservices-*/_search?pretty" -H 'Content-Type: application/json' -d'
{
  "query": {
    "term": {"log_level": "ERROR"}
  },
  "aggs": {
    "errors_by_service": {
      "terms": {
        "field": "service_name",
        "size": 10
      }
    }
  },
  "size": 0
}'

# Find slow requests (>2 seconds)
curl -X GET "localhost:9200/microservices-*/_search?pretty" -H 'Content-Type: application/json' -d'
{
  "query": {
    "range": {
      "duration_seconds": {
        "gt": 2
      }
    }
  },
  "sort": [{"duration_seconds": {"order": "desc"}}],
  "size": 10
}'
```

## Development Commands

### ELK Operations
```bash
# Start ELK stack
docker-compose -f infrastructure/observability/docker-compose.elk.yml up -d

# Check service health
docker-compose -f infrastructure/observability/docker-compose.elk.yml ps

# View logs
docker-compose -f infrastructure/observability/docker-compose.elk.yml logs -f logstash

# Access Kibana
open http://localhost:5601

# Check Elasticsearch cluster health
curl http://localhost:9200/_cluster/health?pretty

# List all indices
curl http://localhost:9200/_cat/indices?v

# Reset Kibana data (if needed)
curl -X DELETE "http://localhost:9200/.kibana*"

# Reload Logstash configuration
docker-compose -f infrastructure/observability/docker-compose.elk.yml restart logstash
```

## Troubleshooting

### Common Issues

#### 1. Elasticsearch Won't Start
```bash
# Check memory settings
echo "vm.max_map_count=262144" | sudo tee -a /etc/sysctl.conf
sudo sysctl -p

# Check disk space
df -h

# Check logs
docker logs elasticsearch
```

#### 2. Logstash Pipeline Errors
```bash
# Check pipeline configuration
docker exec -it logstash /usr/share/logstash/bin/logstash --config.test_and_exit

# View pipeline stats
curl http://localhost:9600/_node/stats/pipelines?pretty

# Debug parsing issues
docker exec -it logstash tail -f /usr/share/logstash/logs/logstash-plain.log
```

#### 3. Missing Logs in Kibana
```bash
# Check index patterns
curl http://localhost:9200/_cat/indices/microservices-*

# Verify Filebeat is running
docker logs filebeat

# Test log ingestion manually
echo "test log entry" >> ./logs/logs.log
```

### Debugging Queries
```json
{
  "query": {
    "bool": {
      "must": [
        {"exists": {"field": "request_id"}},
        {"range": {"@timestamp": {"gte": "now-1h"}}}
      ]
    }
  },
  "aggs": {
    "services": {
      "terms": {"field": "service_name"}
    },
    "log_levels": {
      "terms": {"field": "log_level"}
    }
  }
}
```

## Security Considerations

### Access Control
```yaml
# Enable security (production setup)
xpack.security.enabled: true
xpack.security.transport.ssl.enabled: true
xpack.security.http.ssl.enabled: true

# User roles
elasticsearch_users:
  - username: "kibana_system"
    password: "changeme"
    roles: ["kibana_system"]
  - username: "logstash_writer"
    password: "changeme"
    roles: ["logstash_writer"]
  - username: "readonly_user"
    password: "changeme"
    roles: ["viewer"]
```

### Data Protection
- **Log Sanitization**: Ensure no secrets in logs (leverages `logging_rules.mdc` sanitization)
- **Field Encryption**: Encrypt sensitive fields at rest
- **Access Logs**: Audit access to sensitive log data
- **Retention Policies**: Implement appropriate data retention

---

## Summary

This comprehensive ELK implementation provides powerful log aggregation, analysis, and visualization capabilities for your microservices architecture. It seamlessly integrates with your existing logging foundation from `logging_rules.mdc` and provides production-ready monitoring and alerting.

**Key Features**:
- Complete ELK stack with Filebeat for log collection
- Advanced Logstash pipelines for parsing structured logs from logging_rules.mdc
- Request ID and trace correlation across all logs
- Production-ready Elasticsearch configuration with ILM
- Comprehensive Kibana dashboards and alerting
- Performance optimization and security considerations
- Integration with existing observability stack