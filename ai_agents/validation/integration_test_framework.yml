# Integration Test Framework for Generated Microservices
# Validates that generated services work together correctly

metadata:
  name: "Integration Test Framework"
  version: "1.0.0"
  description: "End-to-end testing framework for generated microservices"

# Test Categories
test_categories:
  # Service-to-Service Communication Tests
  service_communication:
    description: "Test HTTP communication between business and data services"
    test_scenarios:
      api_to_postgres:
        description: "API service communicates with PostgreSQL service"
        test_cases:
          - "Create user via API, verify in PostgreSQL service"
          - "Update user via API, confirm changes in PostgreSQL"
          - "Delete user via API, verify removal in PostgreSQL"

      api_to_mongo:
        description: "API service communicates with MongoDB service"
        test_cases:
          - "Store document via API, verify in MongoDB service"
          - "Query analytics via API, confirm aggregation results"
          - "Update document via API, verify changes in MongoDB"

      bot_to_data_services:
        description: "Bot service communicates with data services"
        test_cases:
          - "Bot command triggers user lookup in PostgreSQL"
          - "Bot interaction logs activity in MongoDB"
          - "Bot notification requires data from both services"

      worker_to_data_services:
        description: "Worker service processes data via services"
        test_cases:
          - "Background task updates PostgreSQL via HTTP"
          - "Analytics worker aggregates data in MongoDB"
          - "Batch processing handles data across both services"

  # Cross-Service Data Flow Tests
  data_flow_tests:
    description: "Test complete data flows across multiple services"
    test_scenarios:
      end_to_end_workflows:
        e_commerce_order_flow:
          steps:
            1: "User creates order via API service"
            2: "API service stores order in PostgreSQL"
            3: "API service publishes order.created event"
            4: "Worker service processes payment"
            5: "Worker service updates inventory in PostgreSQL"
            6: "Worker service logs analytics in MongoDB"
            7: "Bot service sends confirmation to user"
          validation: "Each step completes successfully and data flows correctly"

        content_publishing_flow:
          steps:
            1: "User submits content via API service"
            2: "API service stores metadata in PostgreSQL"
            3: "API service stores content in MongoDB"
            4: "Worker service processes media files"
            5: "Worker service updates search index"
            6: "Bot service notifies team of publication"
          validation: "Content is properly stored and processed"

  # Event-Driven Integration Tests
  event_integration:
    description: "Test RabbitMQ event flow between services"
    test_scenarios:
      event_publishing:
        description: "Services publish events correctly"
        test_cases:
          - "API service publishes events with correct format"
          - "Bot service publishes user interaction events"
          - "Events include proper metadata and timestamps"

      event_consumption:
        description: "Services consume events correctly"
        test_cases:
          - "Worker service consumes and processes events"
          - "Bot service reacts to system events"
          - "Failed event processing triggers retry logic"

      event_ordering:
        description: "Event order and consistency"
        test_cases:
          - "Events are processed in correct order"
          - "Duplicate events are handled properly"
          - "Event delivery guarantees work as expected"

# Test Infrastructure Setup
test_infrastructure:
  # Container-based Testing
  container_testing:
    description: "Use Docker containers for isolated testing"
    setup:
      test_compose:
        file: "docker-compose.test.yml"
        services:
          - "All microservices with test configurations"
          - "Test databases (PostgreSQL, MongoDB)"
          - "Test RabbitMQ instance"
          - "Test Redis cache"

      test_networks:
        isolation: "Separate network for test containers"
        service_discovery: "Services can reach each other by name"

      test_volumes:
        temporary_data: "Use temporary volumes for test data"
        cleanup: "Automatic cleanup after tests"

  # Test Data Management
  test_data:
    description: "Consistent test data across services"
    strategies:
      data_factories:
        user_factory: "Generate realistic user test data"
        product_factory: "Generate product catalog test data"
        order_factory: "Generate order test data"

      data_seeding:
        postgresql_seed: "Seed PostgreSQL with base test data"
        mongodb_seed: "Seed MongoDB with analytics test data"
        consistent_references: "Ensure foreign keys match across services"

      data_cleanup:
        per_test_cleanup: "Clean data between tests"
        transaction_rollback: "Use transactions for isolation"
        container_reset: "Reset containers if needed"

# Test Execution Framework
test_execution:
  # Test Runner Configuration
  test_runner:
    framework: "pytest with asyncio support"
    configuration:
      async_testing: "pytest-asyncio for async test support"
      test_containers: "testcontainers-python for container management"
      http_testing: "httpx for async HTTP client testing"

    test_structure: |
      tests/
      ├── integration/
      │   ├── test_service_communication.py
      │   ├── test_data_flows.py
      │   ├── test_event_integration.py
      │   └── test_complete_workflows.py
      ├── fixtures/
      │   ├── container_fixtures.py
      │   ├── data_fixtures.py
      │   └── client_fixtures.py
      └── conftest.py

  # Test Fixtures and Utilities
  test_fixtures:
    container_fixtures:
      description: "Manage test container lifecycle"
      components:
        - "Start/stop service containers"
        - "Wait for service readiness"
        - "Provide service URLs to tests"

    client_fixtures:
      description: "HTTP clients for service communication"
      components:
        - "Authenticated API client"
        - "Data service clients"
        - "Bot service client (if applicable)"

    data_fixtures:
      description: "Test data setup and teardown"
      components:
        - "Create test users, products, orders"
        - "Set up relationships between entities"
        - "Clean up after each test"

# Specific Test Implementations
test_implementations:
  # Service Communication Tests
  service_communication_tests: |
    import pytest
    import httpx
    from testcontainers.compose import DockerCompose

    @pytest.fixture(scope="session")
    async def services():
        with DockerCompose(".", compose_file_name="docker-compose.test.yml") as compose:
            # Wait for services to be ready
            api_url = f"http://localhost:{compose.get_service_port('api_service', 8000)}"
            postgres_url = f"http://localhost:{compose.get_service_port('db_postgres_service', 8001)}"

            # Wait for health checks
            async with httpx.AsyncClient() as client:
                await wait_for_service(client, f"{api_url}/health")
                await wait_for_service(client, f"{postgres_url}/health")

            yield {
                "api_url": api_url,
                "postgres_url": postgres_url,
            }

    @pytest.mark.asyncio
    async def test_api_creates_user_in_postgres(services):
        """Test that API service can create user via PostgreSQL service"""
        api_url = services["api_url"]

        # Create user via API
        user_data = {
            "email": "test@example.com",
            "name": "Test User"
        }

        async with httpx.AsyncClient() as client:
            response = await client.post(f"{api_url}/users", json=user_data)
            assert response.status_code == 201

            created_user = response.json()
            user_id = created_user["id"]

            # Verify user was created in PostgreSQL service
            postgres_response = await client.get(f"{services['postgres_url']}/users/{user_id}")
            assert postgres_response.status_code == 200

            postgres_user = postgres_response.json()
            assert postgres_user["email"] == user_data["email"]
            assert postgres_user["name"] == user_data["name"]

  # Data Flow Tests
  data_flow_tests: |
    @pytest.mark.asyncio
    async def test_complete_order_workflow(services, test_user, test_product):
        """Test complete e-commerce order workflow"""
        api_url = services["api_url"]

        # Step 1: Create order via API
        order_data = {
            "user_id": test_user["id"],
            "items": [{"product_id": test_product["id"], "quantity": 2}]
        }

        async with httpx.AsyncClient() as client:
            # Create order
            order_response = await client.post(f"{api_url}/orders", json=order_data)
            assert order_response.status_code == 201
            order = order_response.json()

            # Step 2: Verify order in PostgreSQL
            postgres_response = await client.get(f"{services['postgres_url']}/orders/{order['id']}")
            assert postgres_response.status_code == 200

            # Step 3: Wait for background processing (worker service)
            await asyncio.sleep(2)  # Allow time for event processing

            # Step 4: Verify analytics in MongoDB
            mongo_response = await client.get(f"{services['mongo_url']}/analytics/orders/{order['id']}")
            assert mongo_response.status_code == 200

            # Step 5: Verify inventory update
            inventory_response = await client.get(f"{services['postgres_url']}/products/{test_product['id']}/inventory")
            assert inventory_response.status_code == 200
            inventory = inventory_response.json()
            assert inventory["available"] == test_product["initial_stock"] - 2

  # Event Integration Tests
  event_integration_tests: |
    @pytest.mark.asyncio
    async def test_event_publishing_and_consumption(services, rabbitmq_client):
        """Test that events are published and consumed correctly"""
        api_url = services["api_url"]

        # Set up event listener
        events_received = []

        async def event_handler(message):
            events_received.append(json.loads(message.body.decode()))

        # Subscribe to events
        await rabbitmq_client.subscribe("orders.created", event_handler)

        # Trigger event by creating order
        order_data = {"user_id": 1, "items": [{"product_id": 1, "quantity": 1}]}

        async with httpx.AsyncClient() as client:
            response = await client.post(f"{api_url}/orders", json=order_data)
            assert response.status_code == 201

            # Wait for event processing
            await asyncio.sleep(1)

            # Verify event was received
            assert len(events_received) == 1
            event = events_received[0]
            assert event["event_type"] == "order.created"
            assert event["order_id"] == response.json()["id"]

# Performance and Load Testing
performance_testing:
  # Load Testing Scenarios
  load_testing:
    description: "Test system performance under load"
    scenarios:
      concurrent_users:
        description: "Simulate multiple concurrent users"
        metrics:
          - "Response times under load"
          - "Error rates during peak traffic"
          - "Resource utilization"

      data_throughput:
        description: "Test data processing throughput"
        scenarios:
          - "Bulk data insertion performance"
          - "Analytics query performance"
          - "Event processing throughput"

  # Stress Testing
  stress_testing:
    description: "Test system behavior at limits"
    scenarios:
      resource_exhaustion:
        - "Database connection limits"
        - "Memory usage under load"
        - "CPU utilization limits"

      failure_recovery:
        - "Service restart recovery"
        - "Database connection recovery"
        - "Event processing backlog recovery"

# Test Reporting and Monitoring
test_reporting:
  # Test Result Reporting
  reporting:
    format: "JUnit XML + HTML reports"
    metrics:
      - "Test pass/fail rates"
      - "Test execution times"
      - "Coverage reports"
      - "Performance benchmarks"

  # Continuous Integration
  ci_integration:
    triggers:
      - "Run tests on code generation"
      - "Run tests on template changes"
      - "Scheduled integration tests"

    reporting:
      - "Automatic test result notifications"
      - "Performance regression detection"
      - "Integration health dashboard"

# Test Maintenance
test_maintenance:
  # Test Data Management
  data_management:
    strategies:
      - "Automated test data refresh"
      - "Test environment reset procedures"
      - "Data consistency validation"

  # Test Environment Health
  environment_health:
    monitoring:
      - "Container health checks"
      - "Service dependency validation"
      - "Test infrastructure monitoring"

    maintenance:
      - "Regular test environment updates"
      - "Performance baseline updates"
      - "Test cleanup procedures"