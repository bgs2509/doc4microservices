# Integration Patterns for Microservices Architecture
# Defines how services integrate and communicate within the fixed architecture

metadata:
  name: "Service Integration Patterns"
  version: "1.0.0"
  description: "Standard integration patterns for Improved Hybrid Approach microservices"

# Core Integration Principles
integration_principles:
  loose_coupling: "Services are independently deployable and scalable"
  http_communication: "Business services communicate via HTTP APIs only"
  event_driven: "Asynchronous communication via RabbitMQ events"
  service_boundaries: "Clear boundaries between data and business logic services"
  fault_tolerance: "Graceful handling of service failures"

# Service Integration Topology
service_topology:
  data_layer:
    services:
      - name: "db_postgres_service"
        port: 8001
        role: "Transactional data access"
        clients: ["api_service", "bot_service", "worker_service"]
      - name: "db_mongo_service"
        port: 8002
        role: "Document and analytics data access"
        clients: ["api_service", "bot_service", "worker_service"]

  business_layer:
    services:
      - name: "api_service"
        port: 8000
        role: "External API gateway"
        dependencies: ["db_postgres_service", "db_mongo_service"]
      - name: "bot_service"
        port: 8003
        role: "Telegram bot interface"
        dependencies: ["db_postgres_service", "db_mongo_service"]
      - name: "worker_service"
        port: 8004
        role: "Background processing"
        dependencies: ["db_postgres_service", "db_mongo_service"]

  messaging_layer:
    services:
      - name: "rabbitmq"
        port: 5672
        role: "Event messaging broker"
        clients: ["api_service", "bot_service", "worker_service"]

# Integration Patterns
integration_patterns:
  # Pattern 1: Synchronous Request-Response
  sync_request_response:
    description: "Direct HTTP calls for immediate data needs"
    use_cases: ["user authentication", "real-time data queries", "immediate updates"]
    participants: ["business_service", "data_service"]

    communication_flow:
      1: "business_service sends HTTP request"
      2: "data_service processes request"
      3: "data_service returns HTTP response"
      4: "business_service uses response data"

    implementation:
      http_client: "httpx.AsyncClient for non-blocking calls"
      timeout_handling: "Configure request timeouts"
      error_handling: "HTTP status code based error handling"
      retry_logic: "Exponential backoff for failed requests"

    examples:
      user_authentication:
        caller: "api_service"
        target: "db_postgres_service"
        endpoint: "POST /auth/validate"
        flow: "Immediate response required for login"

      real_time_inventory:
        caller: "api_service"
        target: "db_postgres_service"
        endpoint: "GET /products/{id}/inventory"
        flow: "Real-time stock check for purchase"

  # Pattern 2: Asynchronous Event-Driven
  async_event_driven:
    description: "Decoupled communication via events"
    use_cases: ["notifications", "analytics updates", "background processing"]
    participants: ["publisher_service", "rabbitmq", "subscriber_service"]

    communication_flow:
      1: "publisher_service emits event to RabbitMQ"
      2: "RabbitMQ routes event to appropriate queues"
      3: "subscriber_service(s) consume events"
      4: "subscriber_service(s) process events independently"

    implementation:
      message_format: "JSON with schema validation"
      queue_naming: "service.event_type (e.g., orders.created)"
      routing: "Topic-based routing for flexibility"
      durability: "Persistent queues for reliability"

    examples:
      order_processing:
        publisher: "api_service"
        event: "order.created"
        subscribers: ["worker_service", "bot_service"]
        flow: "Order processing and notifications"

      user_activity_tracking:
        publisher: "api_service"
        event: "user.action"
        subscribers: ["worker_service"]
        flow: "Analytics data collection"

  # Pattern 3: Batch Processing
  batch_processing:
    description: "Efficient bulk operations"
    use_cases: ["data migrations", "report generation", "bulk notifications"]
    participants: ["worker_service", "data_services"]

    communication_flow:
      1: "worker_service accumulates batch data"
      2: "worker_service calls bulk API endpoints"
      3: "data_service processes batch efficiently"
      4: "worker_service handles batch results"

    implementation:
      batch_size: "Configurable batch sizes (100-1000 items)"
      parallel_processing: "Multiple workers for large batches"
      error_handling: "Partial failure recovery"
      progress_tracking: "Batch processing status updates"

    examples:
      daily_analytics:
        processor: "worker_service"
        target: "db_mongo_service"
        endpoint: "POST /analytics/bulk_update"
        flow: "Daily analytics computation and storage"

  # Pattern 4: Aggregation Pattern
  data_aggregation:
    description: "Combining data from multiple sources"
    use_cases: ["dashboards", "reports", "complex queries"]
    participants: ["api_service", "multiple_data_services"]

    communication_flow:
      1: "api_service receives request for aggregated data"
      2: "api_service makes parallel calls to data services"
      3: "api_service waits for all responses"
      4: "api_service combines and formats results"

    implementation:
      parallel_requests: "asyncio.gather() for concurrent calls"
      timeout_coordination: "Handle partial timeouts gracefully"
      caching: "Cache aggregated results in Redis"
      fallback_data: "Return partial data if some services fail"

    examples:
      user_dashboard:
        aggregator: "api_service"
        sources: ["db_postgres_service", "db_mongo_service"]
        endpoints: ["/users/{id}", "/analytics/users/{id}"]
        flow: "Combined user profile and analytics data"

# Service Discovery and Configuration
service_discovery:
  static_configuration:
    description: "Services discover each other via environment variables"
    implementation:
      postgres_service_url: "DB_POSTGRES_SERVICE_URL=http://db_postgres_service:8001"
      mongo_service_url: "DB_MONGO_SERVICE_URL=http://db_mongo_service:8002"
      rabbitmq_url: "RABBITMQ_URL=amqp://rabbitmq:5672"

  health_checking:
    description: "Services expose health endpoints for monitoring"
    endpoints:
      - "GET /health - Basic health check"
      - "GET /ready - Readiness probe"
      - "GET /metrics - Prometheus metrics"

# Authentication and Security
security_patterns:
  service_to_service_auth:
    description: "Authentication between internal services"
    implementation:
      api_keys: "Service-specific API keys for data service access"
      jwt_tokens: "Short-lived JWT tokens for request context"
      network_isolation: "Internal Docker network for service communication"

  user_authentication:
    description: "End-user authentication flow"
    flow:
      1: "User provides credentials to api_service"
      2: "api_service validates with db_postgres_service"
      3: "api_service issues JWT token"
      4: "Subsequent requests include JWT token"

# Error Handling and Resilience
resilience_patterns:
  circuit_breaker:
    description: "Prevent cascading failures"
    implementation:
      failure_threshold: "Open circuit after 5 consecutive failures"
      timeout: "30-second timeout before retry"
      fallback: "Return cached data or graceful error"

  retry_mechanism:
    description: "Automatic retry for transient failures"
    strategy:
      immediate_retry: "1 immediate retry"
      exponential_backoff: "2s, 4s, 8s, 16s intervals"
      max_retries: "4 total retry attempts"
      circuit_breaker_integration: "Respect circuit breaker state"

  timeout_handling:
    description: "Request timeout management"
    timeouts:
      fast_queries: "5 seconds for simple CRUD operations"
      complex_queries: "30 seconds for analytics and aggregations"
      bulk_operations: "300 seconds for batch processing"

# Monitoring and Observability
observability_integration:
  distributed_tracing:
    description: "Request tracing across services"
    implementation:
      trace_id: "Generated by api_service, propagated to all calls"
      span_creation: "Each service creates spans for operations"
      jaeger_integration: "Send traces to Jaeger collector"

  metrics_collection:
    description: "Service performance metrics"
    metrics:
      request_count: "Total requests per endpoint"
      response_time: "Request duration histograms"
      error_rate: "Error percentage by service"
      dependency_health: "Health of downstream services"

  structured_logging:
    description: "Consistent logging format"
    log_format:
      timestamp: "ISO 8601 format"
      service_name: "Source service identifier"
      trace_id: "Distributed tracing correlation"
      level: "DEBUG, INFO, WARN, ERROR"
      message: "Human-readable message"

# Integration Testing Patterns
testing_patterns:
  contract_testing:
    description: "API contract validation between services"
    implementation:
      api_schemas: "OpenAPI schemas for all data service endpoints"
      contract_tests: "Validate request/response formats"
      version_compatibility: "Ensure backward compatibility"

  integration_testing:
    description: "End-to-end testing across services"
    approach:
      test_containers: "Use testcontainers for real service instances"
      test_data: "Consistent test data setup and cleanup"
      service_mocking: "Mock external dependencies"

# Deployment Integration
deployment_patterns:
  docker_compose_orchestration:
    description: "Service orchestration via Docker Compose"
    configuration:
      service_dependencies: "Explicit depends_on configuration"
      network_isolation: "Custom Docker networks"
      volume_management: "Persistent data volumes"
      environment_variables: "Service configuration via env vars"

  rolling_updates:
    description: "Zero-downtime service updates"
    strategy:
      health_checks: "Wait for service health before routing traffic"
      graceful_shutdown: "Handle in-flight requests during shutdown"
      database_migrations: "Run migrations before service updates"

# Example Integration Scenarios
integration_examples:
  e_commerce_order_flow:
    description: "Complete order processing integration"
    services_involved: ["api_service", "bot_service", "worker_service", "both_data_services"]
    integration_patterns:
      - "Sync request-response for order creation"
      - "Event-driven processing for payment and fulfillment"
      - "Aggregation for order status dashboard"
      - "Batch processing for daily order reports"

  content_management_workflow:
    description: "Content creation and publishing integration"
    services_involved: ["api_service", "worker_service", "both_data_services"]
    integration_patterns:
      - "Sync request-response for content CRUD"
      - "Event-driven media processing"
      - "Batch processing for content indexing"
      - "Aggregation for content analytics"

  user_analytics_pipeline:
    description: "User behavior tracking and analytics"
    services_involved: ["api_service", "bot_service", "worker_service", "db_mongo_service"]
    integration_patterns:
      - "Event-driven user action tracking"
      - "Batch processing for analytics computation"
      - "Aggregation for dashboard data"
      - "Real-time notifications via bot service"