# Data Flow Patterns for Microservices Architecture
# Defines how data flows between services in the Improved Hybrid Approach

metadata:
  name: "Data Flow Patterns"
  version: "1.0.0"
  description: "Standard data flow patterns for business logic to data service communication"

# Core Data Flow Principles
flow_principles:
  http_only_access: "Business services access data ONLY via HTTP APIs"
  no_direct_db: "No direct database connections from business services"
  service_boundaries: "Clear separation between data and business logic"
  async_communication: "Non-blocking communication patterns"

# Standard Data Flow Patterns
data_flow_patterns:
  # Pattern 1: Simple CRUD Operations
  simple_crud:
    description: "Basic Create, Read, Update, Delete operations"
    participants: ["business_service", "data_service"]
    flow:
      1: "business_service receives user request"
      2: "business_service validates input"
      3: "business_service -> HTTP request -> data_service"
      4: "data_service performs database operation"
      5: "data_service -> HTTP response -> business_service"
      6: "business_service returns formatted response"

    examples:
      create_user:
        business_service: "api_service"
        data_service: "db_postgres_service"
        flow:
          - "POST /api/users (api_service)"
          - "POST /users (db_postgres_service)"
          - "Returns user_id and created user data"

      get_product:
        business_service: "api_service"
        data_service: "db_postgres_service"
        flow:
          - "GET /api/products/{id} (api_service)"
          - "GET /products/{id} (db_postgres_service)"
          - "Returns product details"

  # Pattern 2: Cross-Service Data Aggregation
  data_aggregation:
    description: "Combining data from multiple data services"
    participants: ["business_service", "db_postgres_service", "db_mongo_service"]
    flow:
      1: "business_service receives request"
      2: "business_service -> parallel HTTP requests -> both data services"
      3: "both data services process requests independently"
      4: "business_service aggregates responses"
      5: "business_service returns combined result"

    examples:
      user_dashboard:
        business_service: "api_service"
        data_sources:
          - service: "db_postgres_service"
            data: "user profile, orders, transactions"
          - service: "db_mongo_service"
            data: "user activity, preferences, analytics"
        flow:
          - "GET /api/dashboard/{user_id} (api_service)"
          - "Parallel: GET /users/{user_id} (postgres) + GET /analytics/users/{user_id} (mongo)"
          - "Combine and return dashboard data"

      product_details:
        business_service: "api_service"
        data_sources:
          - service: "db_postgres_service"
            data: "product info, inventory, pricing"
          - service: "db_mongo_service"
            data: "reviews, recommendations, analytics"
        flow:
          - "GET /api/products/{id}/full (api_service)"
          - "Parallel: GET /products/{id} (postgres) + GET /reviews/products/{id} (mongo)"
          - "Return enriched product data"

  # Pattern 3: Event-Driven Data Flow
  event_driven:
    description: "Asynchronous data processing via events"
    participants: ["trigger_service", "worker_service", "data_services", "rabbitmq"]
    flow:
      1: "trigger_service publishes event to RabbitMQ"
      2: "worker_service consumes event from queue"
      3: "worker_service processes event data"
      4: "worker_service -> HTTP requests -> data_services"
      5: "worker_service may publish completion event"

    examples:
      order_processing:
        trigger: "api_service (order created)"
        processor: "worker_service"
        flow:
          - "api_service publishes 'order.created' event"
          - "worker_service consumes event"
          - "worker processes payment, updates inventory, sends notifications"
          - "worker -> HTTP requests to both data services"
          - "worker publishes 'order.completed' event"

      analytics_computation:
        trigger: "api_service (user action)"
        processor: "worker_service"
        flow:
          - "api_service publishes 'user.action' event"
          - "worker_service aggregates analytics data"
          - "worker -> POST /analytics/events (db_mongo_service)"
          - "worker may trigger daily/weekly analytics updates"

  # Pattern 4: Bot Interaction Flow
  bot_interaction:
    description: "Telegram bot user interactions with data"
    participants: ["telegram_user", "bot_service", "data_services"]
    flow:
      1: "user sends command/message to Telegram bot"
      2: "bot_service receives and parses message"
      3: "bot_service -> HTTP requests -> data_services"
      4: "bot_service formats response for Telegram"
      5: "bot_service sends response to user"

    examples:
      check_order_status:
        flow:
          - "User: /order_status 12345"
          - "bot_service -> GET /orders/12345 (db_postgres_service)"
          - "bot_service formats order data"
          - "Bot responds with order status and tracking info"

      quick_task_creation:
        flow:
          - "User: /create_task Fix bug in API"
          - "bot_service -> POST /tasks (db_postgres_service)"
          - "bot_service -> POST /activity_logs (db_mongo_service)"
          - "Bot confirms task creation with ID"

# Data Consistency Patterns
consistency_patterns:
  # Eventually Consistent Pattern
  eventual_consistency:
    description: "Data consistency across services over time"
    use_cases: ["analytics updates", "search index updates", "cache invalidation"]
    implementation:
      - "Primary data stored in appropriate data service"
      - "Secondary data updated asynchronously via events"
      - "Acceptable delay between primary and secondary updates"

    example:
      user_profile_update:
        primary: "db_postgres_service updates user profile"
        secondary: "db_mongo_service updates user analytics profile"
        mechanism: "event-driven update via worker_service"

  # Strong Consistency Pattern
  strong_consistency:
    description: "Immediate consistency for critical operations"
    use_cases: ["financial transactions", "inventory updates", "user authentication"]
    implementation:
      - "Synchronous updates within single data service"
      - "Transaction boundaries within database"
      - "Immediate error handling and rollback"

    example:
      payment_processing:
        operations:
          - "Update order status"
          - "Reduce inventory"
          - "Record payment"
        mechanism: "Single transaction in db_postgres_service"

# Error Handling Patterns
error_handling:
  # Circuit Breaker Pattern
  circuit_breaker:
    description: "Prevent cascading failures between services"
    implementation:
      - "Track failure rates for each data service"
      - "Open circuit after threshold failures"
      - "Fallback to cached data or error response"
      - "Periodically test if service is recovered"

  # Retry Pattern
  retry_with_backoff:
    description: "Retry failed requests with exponential backoff"
    implementation:
      - "Immediate retry for transient errors"
      - "Exponential backoff for subsequent retries"
      - "Maximum retry count to prevent infinite loops"
      - "Dead letter queue for failed messages"

  # Graceful Degradation
  graceful_degradation:
    description: "Provide reduced functionality when services unavailable"
    examples:
      dashboard_with_postgres_down:
        behavior: "Show cached user data, disable real-time updates"
      product_listing_with_mongo_down:
        behavior: "Show basic product info, disable reviews and recommendations"

# Performance Optimization Patterns
performance_patterns:
  # Bulk Operations
  bulk_operations:
    description: "Batch multiple operations for efficiency"
    use_cases: ["data migrations", "batch analytics", "bulk notifications"]
    implementation:
      - "Accumulate operations in batches"
      - "Use bulk API endpoints on data services"
      - "Process in worker_service for non-blocking execution"

  # Caching Strategy
  caching_strategy:
    description: "Redis caching for frequently accessed data"
    layers:
      - "Application-level caching in business services"
      - "Data service response caching"
      - "Database query result caching"
    invalidation: "Event-driven cache invalidation"

  # Parallel Data Access
  parallel_requests:
    description: "Concurrent requests to multiple data services"
    implementation:
      - "Use asyncio.gather() for parallel HTTP requests"
      - "Combine results from multiple data sources"
      - "Handle partial failures gracefully"

# Data Flow Documentation Template
flow_documentation_template:
  flow_name: "string"
  description: "string"
  participants: ["list", "of", "services"]
  trigger: "what initiates the flow"
  steps:
    - step: 1
      action: "description of action"
      service: "which service performs action"
      data: "what data is involved"
  success_outcome: "what happens on success"
  error_handling: "how errors are handled"
  performance_considerations: "optimization notes"

# Example Complete Data Flows
example_flows:
  e_commerce_order_flow:
    description: "Complete order processing from creation to fulfillment"
    participants: ["api_service", "bot_service", "worker_service", "db_postgres_service", "db_mongo_service"]
    steps:
      1:
        action: "User creates order via API"
        service: "api_service"
        data: "order details, user_id, product_ids"
        calls: "POST /orders (db_postgres_service)"
      2:
        action: "Publish order created event"
        service: "api_service"
        data: "order_id, user_id"
        mechanism: "RabbitMQ event"
      3:
        action: "Process payment"
        service: "worker_service"
        data: "payment details"
        calls: "PUT /orders/{id}/payment_status (db_postgres_service)"
      4:
        action: "Update inventory"
        service: "worker_service"
        data: "product quantities"
        calls: "PUT /products/{id}/inventory (db_postgres_service)"
      5:
        action: "Log analytics event"
        service: "worker_service"
        data: "order analytics data"
        calls: "POST /analytics/orders (db_mongo_service)"
      6:
        action: "Send notification to user"
        service: "bot_service"
        data: "order confirmation"
        mechanism: "Telegram message"

  content_publishing_flow:
    description: "Content creation and publishing workflow"
    participants: ["api_service", "worker_service", "db_postgres_service", "db_mongo_service"]
    steps:
      1:
        action: "Author creates content draft"
        service: "api_service"
        data: "content metadata"
        calls: "POST /content_metadata (db_postgres_service)"
      2:
        action: "Store content document"
        service: "api_service"
        data: "content body, media"
        calls: "POST /documents/content (db_mongo_service)"
      3:
        action: "Submit for review"
        service: "api_service"
        data: "workflow status update"
        calls: "PUT /content_metadata/{id}/status (db_postgres_service)"
      4:
        action: "Process media files"
        service: "worker_service"
        data: "media optimization"
        mechanism: "Background processing"
      5:
        action: "Publish content"
        service: "api_service"
        data: "publication status"
        calls: "Multiple updates to both services"
      6:
        action: "Index for search"
        service: "worker_service"
        data: "search index updates"
        calls: "PUT /search_index/content (db_mongo_service)"